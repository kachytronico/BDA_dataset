{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Instalación de Hadoop"],"metadata":{"id":"d3-j-3-td0sZ"}},{"cell_type":"code","source":["# Descargar Hadoop 3.4.2 desde el sitio oficial de Apache\n","!wget https://downloads.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz\n","\n","# Descomprimir el archivo tar.gz descargado\n","!tar -xzf hadoop-3.4.2.tar.gz\n","\n","# Mover la carpeta descomprimida a /usr/local/hadoop\n","!mv hadoop-3.4.2/ /usr/local/hadoop\n","\n","# Establecer la variable de entorno JAVA_HOME para apuntar a la instalación de Java 11\n","import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n","\n","\n","# Agregar el directorio bin de Hadoop al PATH para que los comandos de Hadoop estén disponibles globalmente\n","os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n","os.environ[\"PATH\"] += f\":{os.environ['HADOOP_HOME']}/bin:{os.environ['HADOOP_HOME']}/sbin\"\n","\n","# Verificar la versión de Hadoop instalada\n","!hadoop version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T5WIZHbMo2Ue","executionInfo":{"status":"ok","timestamp":1768217342812,"user_tz":-60,"elapsed":78410,"user":{"displayName":"Javier Rojas Ostiategui","userId":"12409460944585507889"}},"outputId":"a1ccacc1-68f3-406a-aacc-829e31ebd288"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2026-01-12 11:27:44--  https://downloads.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz\n","Resolving downloads.apache.org (downloads.apache.org)... 88.99.208.237, 135.181.214.104, 2a01:4f8:10a:39da::2, ...\n","Connecting to downloads.apache.org (downloads.apache.org)|88.99.208.237|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1065831750 (1016M) [application/x-gzip]\n","Saving to: ‘hadoop-3.4.2.tar.gz’\n","\n","hadoop-3.4.2.tar.gz 100%[===================>]   1016M  18.7MB/s    in 55s     \n","\n","2026-01-12 11:28:40 (18.6 MB/s) - ‘hadoop-3.4.2.tar.gz’ saved [1065831750/1065831750]\n","\n","Hadoop 3.4.2\n","Source code repository https://github.com/apache/hadoop.git -r 84e8b89ee2ebe6923691205b9e171badde7a495c\n","Compiled by ahmarsu on 2025-08-20T10:30Z\n","Compiled on platform linux-x86_64\n","Compiled with protoc 3.23.4\n","From source with checksum fa94c67d4b4be021b9e9515c9b0f7b6\n","This command was run using /usr/local/hadoop/share/hadoop/common/hadoop-common-3.4.2.jar\n"]}]},{"cell_type":"markdown","source":["# SPARK"],"metadata":{"id":"_Giekut_eBDD"}},{"cell_type":"markdown","source":["- [Apache Spark](https://spark.apache.org) se lanzó por primera vez en 2014.\n","- Fue desarrollado originalmente por [Matei Zaharia](http://people.csail.mit.edu/matei) como un proyecto de clase, y más tarde una tesis doctoral, en la Universidad de California, Berkeley.\n","- Spark está escrito en [Scala](https://www.scala-lang.org).\n","- Todas las imágenes proceden de [Databricks](https://databricks.com/product/getting-started-guide).\n","- Apache Spark es un sistema de computación en clúster rápido y de propósito general.\n","- Proporciona API de alto nivel en Java, Scala, Python y R, y un motor optimizado que soporta gráficos de ejecución general.\n","- Spark puede gestionar colecciones de \"big data\" con un pequeño conjunto de primitivas de alto nivel como `map`, `filter`, `groupby` y `join`.  Con estos patrones comunes a menudo podemos manejar cálculos que son más complejos que map, pero siguen siendo estructurados.\n","- También es compatible con un amplio conjunto de herramientas de alto nivel, como [Spark SQL](https://spark.apache.org/docs/latest/sql-programming-guide.html) para SQL y el procesamiento de datos estructurados, [MLlib](https://spark.apache.org/docs/latest/ml-guide.html) para el aprendizaje automático, [GraphX](https://spark.apache.org/docs/latest/graphx-programming-guide.html) para el procesamiento de gráficos y Spark Streaming."],"metadata":{"id":"wQwmLEcbHPnu"}},{"cell_type":"markdown","source":["## Ciclo de vida de un programa Spark\n","\n","1. Crea algunos RDD de entrada a partir de datos externos o paraleliza una colección en tu programa controlador.\n","2. Transformarlos perezosamente para definir nuevos RDDs usando transformaciones como `filter()` o `map()`.\n","3. Pedir a Spark que almacene en caché() cualquier RDD intermedio que deba ser reutilizado.\n","4. Lanzar acciones como count() y collect() para iniciar un cálculo paralelo, que luego es optimizado y ejecutado por Spark.\n"],"metadata":{"id":"XhhfuEl6Hung"}},{"cell_type":"markdown","source":["## Operaciones con datos distribuidos\n","\n","- Dos tipos de operaciones: **transformaciones** y **acciones**.\n","- Las transformaciones son *lazy* (no se calculan inmediatamente)\n","- Las transformaciones se ejecutan cuando se ejecuta una acción"],"metadata":{"id":"VZS9-7EnIOnK"}},{"cell_type":"markdown","source":["## [Transformaciones](https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations) (lazy)\n","\n","```\n","map() flatMap()\n","filter()\n","mapPartitions() mapPartitionsWithIndex()\n","sample()\n","union() intersection() distinct()\n","groupBy() groupByKey()\n","reduceBy() reduceByKey()\n","sortBy() sortByKey()\n","join()\n","cogroup()\n","cartesian()\n","pipe()\n","coalesce()\n","repartition()\n","partitionBy()\n","...\n","```"],"metadata":{"id":"SigIRrQAIWep"}},{"cell_type":"markdown","source":["## [Acciones](https://spark.apache.org/docs/latest/rdd-programming-guide.html#actions)\n","\n","```\n","reduce()\n","collect()\n","count()\n","first()\n","take()\n","takeSample()\n","saveToCassandra()\n","takeOrdered()\n","saveAsTextFile()\n","saveAsSequenceFile()\n","saveAsObjectFile()\n","countByKey()\n","foreach()\n","```"],"metadata":{"id":"nqZvEFpGIgMs"}},{"cell_type":"markdown","source":["## Python API\n","\n","PySpark utiliza Py4J, que permite a los programas Python acceder dinámicamente a objetos Java.\n","\n","![picture](https://drive.google.com/uc?export=view&id=1Llin_Zd-11YhHrRds6l_M3GAJWU3x2Ly)\n"],"metadata":{"id":"x7VEPoqgInjt"}},{"cell_type":"markdown","source":["## La clase SparkContext\n","\n","- Cuando trabajamos con Apache Spark invocamos métodos sobre un objeto que es una instancia del contexto `pyspark.SparkContext`.\n","\n","- Típicamente, una instancia de este objeto se crea automáticamente y se asigna a la variable `sc`.\n","\n","- El método `parallelize` de `SparkContext` se puede utilizar para convertir cualquier colección ordinaria de Python en un RDD;\n","    - normalmente crearíamos un RDD a partir de un archivo grande o una tabla HBase."],"metadata":{"id":"tdVKAOB3KxIs"}},{"cell_type":"markdown","metadata":{"id":"2I7p_uqUiIb3"},"source":["## Instalación"]},{"cell_type":"markdown","metadata":{"id":"ri7MiwC8fOqi"},"source":["En primer lugar instalamos y configuramos todas las dependencias de Spark para Python. De esta forma enlazaremos nuestro entorno con el servidor de Spark. Además configuraremos el entorno Spark con las variables que sean necesarias.\n","\n","\n","\n","**NOTA:**\n","\n","1. **la última versión de PySpark es la 4.4.1 [link](https://pypi.org/project/pyspark/#history)**\n","\n","2. **Puede tardar un poco tiempo en hacer todos los procesos de descarga de datos pyspark tardar en descargar en su entorno virtual**\n"]},{"cell_type":"markdown","source":["* Descargamos y comprimimos la instalación de Spark\n"],"metadata":{"id":"JJ93-T-7SLhh"}},{"cell_type":"code","metadata":{"id":"XWM4ntmGfLSE"},"source":["# Install spark-related dependencies\n","!wget -q https://dlcdn.apache.org/spark/spark-4.1.1/spark-4.1.1-bin-hadoop3.tgz\n","!tar xf spark-4.1.1-bin-hadoop3.tgz\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install -q findspark\n","!pip install pyspark\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RNwYkLIBSJuK","outputId":"c807018c-0b78-40bb-9d56-557d282d6ab7","executionInfo":{"status":"ok","timestamp":1768218311294,"user_tz":-60,"elapsed":11502,"user":{"displayName":"Javier Rojas Ostiategui","userId":"12409460944585507889"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pyspark in /usr/local/lib/python3.12/dist-packages (4.0.1)\n","Requirement already satisfied: py4j==0.10.9.9 in /usr/local/lib/python3.12/dist-packages (from pyspark) (0.10.9.9)\n"]}]},{"cell_type":"markdown","source":["* Configurar las variables de entorno"],"metadata":{"id":"UtXmFZKXjZJY"}},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-4.1.1-bin-hadoop3\""],"metadata":{"id":"dVArSVXhjVN-"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6dq4j8GfmSRY"},"source":["Vamos a verificar que el entorno Spark está bien creado"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"4pMTURnTgoIe","outputId":"f76b1edd-e02f-4a78-ea46-1e38c7b1113c","executionInfo":{"status":"ok","timestamp":1768218323516,"user_tz":-60,"elapsed":6,"user":{"displayName":"Javier Rojas Ostiategui","userId":"12409460944585507889"}}},"source":["os.environ[\"SPARK_HOME\"]"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'/content/spark-4.1.1-bin-hadoop3'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","metadata":{"id":"CCyZxBpShBiP"},"source":["Vamos a iniciar uan sesión de spark simple para testear nuestra instalación\n","\n","1. Ejecutamos findspark.init() para hacer que pyspark sea importable como una biblioteca normal"]},{"cell_type":"code","metadata":{"id":"Y7nrYBCBhBOh"},"source":["import findspark\n","findspark.init()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8LZqWpiYnpGt"},"source":["2. Crear un contexto Spark para ejecutar la aplicación\n","\n","> NOTA: Un SparkContext representa la conexión al cluster de Spark, y puede utilizarse para crear RDDs y otros elementos. **Sólo puede haber un SparkContext activo**. Se debe detener (*stop()*) el SparkContext activo antes de crear uno nuevo."]},{"cell_type":"code","metadata":{"id":"8E2ccfuQWsug"},"source":["import pyspark # Importa la librería principal de PySpark para trabajar con Spark.\n","from pyspark.sql import *  # Importa todas las clases y funciones del módulo pyspark.sql para trabajar con DataFrames.\n","from pyspark.sql.functions import * # Importa todas las funciones de manipulación de DataFrames, como agregaciones, filtros, etc.\n","from pyspark import SparkContext, SparkConf # Importa las clases SparkContext y SparkConf para configurar y crear un contexto de Spark."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jMbWlE-0nbJl","colab":{"base_uri":"https://localhost:8080/","height":196},"outputId":"335a36ec-7006-4d9b-f6af-cef335ae0738","executionInfo":{"status":"ok","timestamp":1768218473783,"user_tz":-60,"elapsed":15857,"user":{"displayName":"Javier Rojas Ostiategui","userId":"12409460944585507889"}}},"source":["conf = SparkConf() # Crea una instancia de la clase SparkConf para configurar el contexto de Spark.\n","conf.set(\"spark.ui.port\", \"4050\") # Establece el puerto para la interfaz de usuario de Spark en 4050.\n","conf.set(\"spark.appName\", \"Pi\") # Establece el nombre de la aplicación Spark como \"Pi\".\n","# Crea el contexto\n","sc = pyspark.SparkContext(conf=conf) # Crea un SparkContext utilizando la configuración especificada en 'conf'.\n","spark = SparkSession.builder.getOrCreate() # Crea o obtiene una sesión Spark, que es el punto de entrada para usar DataFrames y SQL.\n","sc # Devuelve el SparkContext creado.\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<SparkContext master=local[*] appName=pyspark-shell>"],"text/html":["\n","        <div>\n","            <p><b>SparkContext</b></p>\n","\n","            <p><a href=\"http://4b370afaf2c0:4050\">Spark UI</a></p>\n","\n","            <dl>\n","              <dt>Version</dt>\n","                <dd><code>v4.1.1</code></dd>\n","              <dt>Master</dt>\n","                <dd><code>local[*]</code></dd>\n","              <dt>AppName</dt>\n","                <dd><code>pyspark-shell</code></dd>\n","            </dl>\n","        </div>\n","        "]},"metadata":{},"execution_count":15}]},{"cell_type":"markdown","metadata":{"id":"QeZ3vsFxpMmT"},"source":["3. Plantear el problema a resolver. En este caso la aproximación de Pi por el método de Montecarlo [enlace](https://www.geogebra.org/m/cF7RwK3H)\n"]},{"cell_type":"code","metadata":{"id":"1lcoGXBzkU5s"},"source":["import random # Importa la librería 'random' para generar números aleatorios.\n","num_samples = 1000000 # Define el número de muestras a utilizar en la simulación de Monte Carlo.\n","def inside(p): # Define una función llamada 'inside' que determina si un punto está dentro del círculo unitario.\n","  x, y = random.random(), random.random() # Genera dos números aleatorios entre 0 y 1, representando las coordenadas x e y de un punto.\n","  return x*x + y*y < 1 # Devuelve True si el punto está dentro del círculo (x^2 + y^2 < 1), False en caso contrario.\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eRxOanjhpCoJ"},"source":["4. Paralelizar el cálculo con **parallelize**: Este método se utiliza para distribuir la colección de elementos del mismo tipo (datos u operaciones) para poder funcionar en paralelo."]},{"cell_type":"code","metadata":{"id":"JzoJEvPtoUNB","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cabe3f40-d334-4e74-f713-5096b5bcd611","executionInfo":{"status":"ok","timestamp":1768218483821,"user_tz":-60,"elapsed":3521,"user":{"displayName":"Javier Rojas Ostiategui","userId":"12409460944585507889"}}},"source":["# Paralelizar el cálculo con parallelize: Este método se utiliza para distribuir la colección de elementos del mismo tipo (datos u operaciones) para poder funcionar en paralelo.\n","count = sc.parallelize(range(0, num_samples)).filter(inside).count() # Crea un RDD con un rango de números, filtra los que están dentro del círculo y cuenta cuántos hay.\n","count\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["785348"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","metadata":{"id":"po4pk0sJprwq"},"source":["5. Obtenemos el resultado final"]},{"cell_type":"code","metadata":{"id":"672qv1phpq8g","colab":{"base_uri":"https://localhost:8080/"},"outputId":"322038b8-6324-4c98-a001-8f562b5645c4","executionInfo":{"status":"ok","timestamp":1768218487627,"user_tz":-60,"elapsed":7,"user":{"displayName":"Javier Rojas Ostiategui","userId":"12409460944585507889"}}},"source":["pi = 4 * count / num_samples # Calcula una aproximación de pi utilizando la fórmula: 4 * (puntos dentro del círculo) / (total de puntos).\n","print(pi) # Imprime el valor aproximado de pi.\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["3.141392\n"]}]},{"cell_type":"markdown","metadata":{"id":"DBUPnpdIoWRu"},"source":["6. Paramos el SparkContext"]},{"cell_type":"code","metadata":{"id":"kB9mF_zJoVal"},"source":["sc.stop() # Detiene el contexto de Spark, liberando los recursos utilizados."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["---\n","\n","# Tarea de refuerzo\n","\n","---"],"metadata":{"id":"2tlJeMSOJAjP"}},{"cell_type":"markdown","source":["Estudia prueba y guarda los siguientes comandos de Pyspark:\n","https://github.com/joeyism/Commonly-Used-Pyspark-Commands\n","\n","Seguro que te serán útilies en el futuro."],"metadata":{"id":"HJZ2qr92I_Ro"}}]}