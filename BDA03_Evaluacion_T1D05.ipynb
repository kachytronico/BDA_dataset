{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "x3zaYPoS3wFz",
   "metadata": {
    "id": "x3zaYPoS3wFz"
   },
   "source": [
    "# BDA03 ‚Äî Tarea de Evaluaci√≥n (Ecosistema Hadoop‚ÄìSpark)\n",
    "**Flujo ETL completo:** importaci√≥n autom√°tica ‚Üí exploraci√≥n ‚Üí limpieza (Pig) ‚Üí transformaci√≥n ‚Üí carga en Hive ‚Üí consultas HQL con JOIN.\n",
    "\n",
    "**Dataset:** T1D (Guertin et al., 2024) ‚Äî Virginia PrIMeD  \n",
    "**Archivos:** `survey_data_and_results_final.xlsx` + `assay_final_genotyping_file.xlsx`  \n",
    "**Clave de relaci√≥n esperada (a validar):** `survey.SUBJECT_ID` ‚Üî `genotyping.FID`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Bs6utBjrAnxZ",
   "metadata": {
    "id": "Bs6utBjrAnxZ"
   },
   "source": [
    "## Explicaci√≥n del conjunto de datos elegido\n",
    "He elegido un dataset real sobre **riesgo gen√©tico de Diabetes Tipo 1 (T1D)** del estudio **Guertin et al. (2024)** (cohorte Virginia PrIMeD). Es interesante porque permite combinar datos **cl√≠nicos/demogr√°ficos** con datos **gen√©ticos**, lo cual es un escenario t√≠pico de integraci√≥n (JOIN) en Big Data.\n",
    "\n",
    "### Archivos utilizados\n",
    "Trabajo con **dos archivos interrelacionados**:\n",
    "\n",
    "1) **`survey_data_and_results_final.xlsx` (Survey / Fenotipo)**  \n",
    "Contiene informaci√≥n del participante (cl√≠nica/demogr√°fica) y variables relacionadas con riesgo gen√©tico.  \n",
    "**Clave:** `SUBJECT_ID`.\n",
    "\n",
    "2) **`assay_final_genotyping_file.xlsx` (Genotyping / Gen√©tica)**  \n",
    "Contiene marcadores gen√©ticos (SNPs) por participante.  \n",
    "**Clave:** `FID`.\n",
    "\n",
    "### Relaci√≥n entre archivos\n",
    "La relaci√≥n se realiza por el identificador de participante:\n",
    "- **`survey.SUBJECT_ID = genotyping.FID`**  \n",
    "(Validar√© esta relaci√≥n con datos reales antes de continuar.)\n",
    "\n",
    "### Tama√±o y relevancia\n",
    "El dataset tiene en torno a **3.800 participantes**, suficiente para demostrar un flujo ETL completo con Hadoop (Pig) y Spark/Hive, incluyendo limpieza de datos, transformaci√≥n y consultas con JOIN.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CTNHxl5fvZsr",
   "metadata": {
    "id": "CTNHxl5fvZsr"
   },
   "source": [
    "# 1. Importaci√≥n autom√°tica + conversi√≥n a CSV\n",
    "\n",
    "\n",
    "> Objetivo: traer los datos de forma reproducible (sin subida manual) y dejarlos en CSV para Pig/Spark/Hive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ux6EPKfEA1uN",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 961
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8245,
     "status": "ok",
     "timestamp": 1771447223986,
     "user": {
      "displayName": "Alfredo Ledesma Ruiz",
      "userId": "02263706330541384863"
     },
     "user_tz": -60
    },
    "id": "ux6EPKfEA1uN",
    "outputId": "41bd376c-e727-42f0-9a4f-97e82cc070d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'BDA_dataset'...\n",
      "remote: Enumerating objects: 89, done.\u001b[K\n",
      "remote: Counting objects: 100% (89/89), done.\u001b[K\n",
      "remote: Compressing objects: 100% (69/69), done.\u001b[K\n",
      "remote: Total 89 (delta 21), reused 79 (delta 14), pack-reused 0 (from 0)\u001b[K\n",
      "Receiving objects: 100% (89/89), 3.76 MiB | 8.83 MiB/s, done.\n",
      "Resolving deltas: 100% (21/21), done.\n",
      "üìÅ Repo clonado (muestra):\n",
      "total 132K\n",
      "drwxr-xr-x 3 root root 4.0K Feb 18 20:40 BDA03_cuadernos\n",
      "-rw-r--r-- 1 root root 6.1K Feb 18 20:40 BDA03_Evaluacion_T1D01.ipynb\n",
      "-rw-r--r-- 1 root root  60K Feb 18 20:40 BDA03_Evaluacion_T1D_copilot.ipynb\n",
      "-rw-r--r-- 1 root root  41K Feb 18 20:40 BDA03_Evaluacion_T1D.ipynb\n",
      "drwxr-xr-x 2 root root 4.0K Feb 18 20:40 Dataset not incorporated into the T1DKP\n",
      "drwxr-xr-x 4 root root 4.0K Feb 18 20:40 docs\n",
      "-rw-r--r-- 1 root root 7.5K Feb 18 20:40 README.md\n",
      "\n",
      "üìÅ RAW:\n",
      "total 1.7M\n",
      "-rw-r--r-- 1 root root 1.4M Feb 18 20:40 assay_final_genotyping_file.xlsx\n",
      "-rw-r--r-- 1 root root 295K Feb 18 20:40 survey_data_and_results_final.xlsx\n",
      "\n",
      "üìÅ CSV:\n",
      "total 1.8M\n",
      "-rw-r--r-- 1 root root 1.2M Feb 18 20:40 genotyping.csv\n",
      "-rw-r--r-- 1 root root 528K Feb 18 20:40 survey.csv\n",
      "\n",
      "shape survey: (3818, 15)\n",
      "shape genotyping: (3818, 76)\n",
      "\n",
      "Preview survey (3 filas):\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"display(df_geno\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"SUBJECT_ID\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 5773503189,\n        \"min\": 10011708520314,\n        \"max\": 10021708521587,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          10011708520314,\n          10021708520764,\n          10021708521587\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AGE\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 2,\n        \"min\": 3,\n        \"max\": 7,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          6,\n          3,\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"RACE\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Asian\",\n          \"White\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"T1D_HIST\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Don't know\",\n          \"Yes\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AUTO_HIST\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Yes\",\n          \"No\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AUTO_COND\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Thyroid_Hashimotos and_or Graves, Blood relative has been diagnosed with autoimmune disease but I don't know which condition\",\n          \"Not applicable\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AUTO_COND_4_TEXT\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Not applicable\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"T1D_DIAG\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"No\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"T1D_DIAG_AGE\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Not applicable\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"T1D_HOSP\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Not applicable\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"DKA\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Not applicable\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GRS_HLA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.200709318914635,\n        \"min\": -13.66,\n        \"max\": 1.91,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.91\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GnonHLA\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.9079831129119821,\n        \"min\": 0.14,\n        \"max\": 1.93,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.14\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"GRS\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 8.222957699838503,\n        \"min\": -12.89,\n        \"max\": 2.06,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          2.06\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Risk\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Not high\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-85d54601-c909-4274-9eec-6a39811286d2\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>AGE</th>\n",
       "      <th>RACE</th>\n",
       "      <th>T1D_HIST</th>\n",
       "      <th>AUTO_HIST</th>\n",
       "      <th>AUTO_COND</th>\n",
       "      <th>AUTO_COND_4_TEXT</th>\n",
       "      <th>T1D_DIAG</th>\n",
       "      <th>T1D_DIAG_AGE</th>\n",
       "      <th>T1D_HOSP</th>\n",
       "      <th>DKA</th>\n",
       "      <th>GRS_HLA</th>\n",
       "      <th>GnonHLA</th>\n",
       "      <th>GRS</th>\n",
       "      <th>Risk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011708520314</td>\n",
       "      <td>6</td>\n",
       "      <td>White</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>No</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>1.91</td>\n",
       "      <td>0.14</td>\n",
       "      <td>2.06</td>\n",
       "      <td>Not high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10021708520764</td>\n",
       "      <td>3</td>\n",
       "      <td>White</td>\n",
       "      <td>Don't know</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Thyroid_Hashimotos and_or Graves, Blood relati...</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>No</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>-1.41</td>\n",
       "      <td>1.93</td>\n",
       "      <td>0.52</td>\n",
       "      <td>Not high</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10021708521587</td>\n",
       "      <td>7</td>\n",
       "      <td>Asian</td>\n",
       "      <td>Don't know</td>\n",
       "      <td>No</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>No</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>Not applicable</td>\n",
       "      <td>-13.66</td>\n",
       "      <td>0.77</td>\n",
       "      <td>-12.89</td>\n",
       "      <td>Not high</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-85d54601-c909-4274-9eec-6a39811286d2')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-85d54601-c909-4274-9eec-6a39811286d2 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-85d54601-c909-4274-9eec-6a39811286d2');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       SUBJECT_ID  AGE   RACE    T1D_HIST AUTO_HIST  \\\n",
       "0  10011708520314    6  White         Yes        No   \n",
       "1  10021708520764    3  White  Don't know       Yes   \n",
       "2  10021708521587    7  Asian  Don't know        No   \n",
       "\n",
       "                                           AUTO_COND AUTO_COND_4_TEXT  \\\n",
       "0                                     Not applicable   Not applicable   \n",
       "1  Thyroid_Hashimotos and_or Graves, Blood relati...   Not applicable   \n",
       "2                                     Not applicable   Not applicable   \n",
       "\n",
       "  T1D_DIAG    T1D_DIAG_AGE        T1D_HOSP             DKA  GRS_HLA  GnonHLA  \\\n",
       "0       No  Not applicable  Not applicable  Not applicable     1.91     0.14   \n",
       "1       No  Not applicable  Not applicable  Not applicable    -1.41     1.93   \n",
       "2       No  Not applicable  Not applicable  Not applicable   -13.66     0.77   \n",
       "\n",
       "     GRS      Risk  \n",
       "0   2.06  Not high  \n",
       "1   0.52  Not high  \n",
       "2 -12.89  Not high  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preview genotyping (3 filas):\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-e193b29e-a756-4878-9ffd-62edb1dd2e17\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID</th>\n",
       "      <th>contact key</th>\n",
       "      <th>rs1049225</th>\n",
       "      <th>rs1052553</th>\n",
       "      <th>rs10795791</th>\n",
       "      <th>rs11203203</th>\n",
       "      <th>rs113010081</th>\n",
       "      <th>rs1150743</th>\n",
       "      <th>rs12416116</th>\n",
       "      <th>rs12720356</th>\n",
       "      <th>...</th>\n",
       "      <th>rs757411</th>\n",
       "      <th>rs7745656</th>\n",
       "      <th>rs7780389</th>\n",
       "      <th>rs917911</th>\n",
       "      <th>rs9268633</th>\n",
       "      <th>rs9271366</th>\n",
       "      <th>rs9273363</th>\n",
       "      <th>rs9357152</th>\n",
       "      <th>rs9469341</th>\n",
       "      <th>rs9585056</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011708520314</td>\n",
       "      <td>CONTACT10085</td>\n",
       "      <td>C:C</td>\n",
       "      <td>A:A</td>\n",
       "      <td>A:G</td>\n",
       "      <td>A:G</td>\n",
       "      <td>T:T</td>\n",
       "      <td>A:G</td>\n",
       "      <td>C:C</td>\n",
       "      <td>T:T</td>\n",
       "      <td>...</td>\n",
       "      <td>C:T</td>\n",
       "      <td>G:T</td>\n",
       "      <td>C:C</td>\n",
       "      <td>G:T</td>\n",
       "      <td>A:G</td>\n",
       "      <td>A:A</td>\n",
       "      <td>A:C</td>\n",
       "      <td>A:A</td>\n",
       "      <td>A:G</td>\n",
       "      <td>T:T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10021708520764</td>\n",
       "      <td>CONTACT14053</td>\n",
       "      <td>C:C</td>\n",
       "      <td>A:A</td>\n",
       "      <td>G:G</td>\n",
       "      <td>A:A</td>\n",
       "      <td>T:T</td>\n",
       "      <td>G:G</td>\n",
       "      <td>C:C</td>\n",
       "      <td>T:T</td>\n",
       "      <td>...</td>\n",
       "      <td>C:C</td>\n",
       "      <td>T:T</td>\n",
       "      <td>C:C</td>\n",
       "      <td>G:T</td>\n",
       "      <td>G:G</td>\n",
       "      <td>A:A</td>\n",
       "      <td>C:C</td>\n",
       "      <td>A:G</td>\n",
       "      <td>A:G</td>\n",
       "      <td>C:T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10021708521587</td>\n",
       "      <td>CONTACT11350</td>\n",
       "      <td>C:C</td>\n",
       "      <td>A:G</td>\n",
       "      <td>A:G</td>\n",
       "      <td>G:G</td>\n",
       "      <td>T:T</td>\n",
       "      <td>A:A</td>\n",
       "      <td>A:A</td>\n",
       "      <td>T:T</td>\n",
       "      <td>...</td>\n",
       "      <td>C:T</td>\n",
       "      <td>T:T</td>\n",
       "      <td>C:C</td>\n",
       "      <td>T:T</td>\n",
       "      <td>A:A</td>\n",
       "      <td>G:G</td>\n",
       "      <td>C:C</td>\n",
       "      <td>A:A</td>\n",
       "      <td>G:G</td>\n",
       "      <td>C:T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows √ó 76 columns</p>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e193b29e-a756-4878-9ffd-62edb1dd2e17')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-e193b29e-a756-4878-9ffd-62edb1dd2e17 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-e193b29e-a756-4878-9ffd-62edb1dd2e17');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "              FID   contact key rs1049225 rs1052553 rs10795791 rs11203203  \\\n",
       "0  10011708520314  CONTACT10085       C:C       A:A        A:G        A:G   \n",
       "1  10021708520764  CONTACT14053       C:C       A:A        G:G        A:A   \n",
       "2  10021708521587  CONTACT11350       C:C       A:G        A:G        G:G   \n",
       "\n",
       "  rs113010081 rs1150743 rs12416116 rs12720356  ... rs757411 rs7745656  \\\n",
       "0         T:T       A:G        C:C        T:T  ...      C:T       G:T   \n",
       "1         T:T       G:G        C:C        T:T  ...      C:C       T:T   \n",
       "2         T:T       A:A        A:A        T:T  ...      C:T       T:T   \n",
       "\n",
       "  rs7780389 rs917911 rs9268633 rs9271366 rs9273363 rs9357152 rs9469341  \\\n",
       "0       C:C      G:T       A:G       A:A       A:C       A:A       A:G   \n",
       "1       C:C      G:T       G:G       A:A       C:C       A:G       A:G   \n",
       "2       C:C      T:T       A:A       G:G       C:C       A:A       G:G   \n",
       "\n",
       "  rs9585056  \n",
       "0       T:T  \n",
       "1       C:T  \n",
       "2       C:T  \n",
       "\n",
       "[3 rows x 76 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. Importaci√≥n autom√°tica + conversi√≥n a CSV\n",
    "# Objetivo: traer los datos de forma reproducible (sin subida manual) y dejarlos en CSV para Pig/Spark/Hive.\n",
    "\n",
    "import os, shutil\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Importaci√≥n autom√°tica: clonar repo\n",
    "!rm -rf BDA_dataset\n",
    "!git clone https://github.com/kachytronico/BDA_dataset\n",
    "\n",
    "# 2) Rutas reales a los XLSX dentro del repo clonado\n",
    "base_path = \"/content/BDA_dataset/Dataset not incorporated into the T1DKP\"\n",
    "survey_xlsx = os.path.join(base_path, \"survey_data_and_results_final.xlsx\")\n",
    "geno_xlsx   = os.path.join(base_path, \"assay_final_genotyping_file.xlsx\")\n",
    "\n",
    "assert os.path.exists(survey_xlsx), f\"Falta archivo requerido: {survey_xlsx}\"\n",
    "assert os.path.exists(geno_xlsx),   f\"Falta archivo requerido: {geno_xlsx}\"\n",
    "\n",
    "# 3) Copia a RAW (mantener originales intactos)\n",
    "raw_dir = \"/content/data/raw\"\n",
    "csv_dir = \"/content/data/csv\"\n",
    "os.makedirs(raw_dir, exist_ok=True)\n",
    "os.makedirs(csv_dir, exist_ok=True)\n",
    "\n",
    "survey_raw = os.path.join(raw_dir, \"survey_data_and_results_final.xlsx\")\n",
    "geno_raw   = os.path.join(raw_dir, \"assay_final_genotyping_file.xlsx\")\n",
    "shutil.copy2(survey_xlsx, survey_raw)\n",
    "shutil.copy2(geno_xlsx, geno_raw)\n",
    "\n",
    "# 4) Convertir a CSV (formato base para Pig/Spark/Hive)\n",
    "df_survey = pd.read_excel(survey_raw)\n",
    "df_geno   = pd.read_excel(geno_raw)\n",
    "\n",
    "survey_csv = os.path.join(csv_dir, \"survey.csv\")\n",
    "geno_csv   = os.path.join(csv_dir, \"genotyping.csv\")\n",
    "df_survey.to_csv(survey_csv, index=False)\n",
    "df_geno.to_csv(geno_csv, index=False)\n",
    "\n",
    "# 5) Evidencia m√≠nima visible\n",
    "print(\"üìÅ Repo clonado (muestra):\")\n",
    "!ls -lh BDA_dataset | head -n 20\n",
    "\n",
    "print(\"\\nüìÅ RAW:\")\n",
    "!ls -lh /content/data/raw\n",
    "\n",
    "print(\"\\nüìÅ CSV:\")\n",
    "!ls -lh /content/data/csv\n",
    "\n",
    "print(\"\\nshape survey:\", df_survey.shape)\n",
    "print(\"shape genotyping:\", df_geno.shape)\n",
    "\n",
    "print(\"\\nPreview survey (3 filas):\")\n",
    "display(df_survey.head(3))\n",
    "\n",
    "print(\"\\nPreview genotyping (3 filas):\")\n",
    "display(df_geno.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eC9xOD1KN9kS",
   "metadata": {
    "id": "eC9xOD1KN9kS"
   },
   "source": [
    "## Conclusiones (Apartado 1)\n",
    "- He importado los datos de forma autom√°tica clonando el repositorio con `git clone`, sin subida manual.\n",
    "- He conservado los originales en RAW (295 KB survey y 1.4 MB genotyping) y he convertido ambos a CSV (528 KB y 1.2 MB) para poder trabajar despu√©s con Pig y Spark/Hive.\n",
    "- He verificado que ambos datasets tienen **3818 filas**; la estructura final es **15 columnas** en survey y **76 columnas** en genotyping.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "GNB32bF6xgWz",
   "metadata": {
    "id": "GNB32bF6xgWz"
   },
   "source": [
    "# 2. Exploraci√≥n con Pandas (usando CSV convertidos)\n",
    "\n",
    "\n",
    "\n",
    "> Objetivo: validar JOIN real y detectar problemas de calidad (nulos, -9) sobre los CSV.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "kIpv02eiDphS",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1078
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1771447224018,
     "user": {
      "displayName": "Alfredo Ledesma Ruiz",
      "userId": "02263706330541384863"
     },
     "user_tz": -60
    },
    "id": "kIpv02eiDphS",
    "outputId": "425d727e-7956-45d6-94e0-776761254e57"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDs √∫nicos survey (SUBJECT_ID): 3818\n",
      "IDs √∫nicos genotyping (FID):   3818\n",
      "Intersecci√≥n IDs: 3818\n",
      "Filas merge inner: 3818\n"
     ]
    },
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "summary": "{\n  \"name\": \"print(\\\"\\\\nTotal de valores '-9' (string) en genotyping:\\\", int(minus9_total))\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"SUBJECT_ID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"10011708520314\",\n          \"10021708520764\",\n          \"10021708521587\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"FID\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"10011708520314\",\n          \"10021708520764\",\n          \"10021708521587\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
       "type": "dataframe"
      },
      "text/html": [
       "\n",
       "  <div id=\"df-f8fa72ce-c5aa-40bd-ad7d-fed98cee7686\" class=\"colab-df-container\">\n",
       "    <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <th>FID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10011708520314</td>\n",
       "      <td>10011708520314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10021708520764</td>\n",
       "      <td>10021708520764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10021708521587</td>\n",
       "      <td>10021708521587</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>\n",
       "    <div class=\"colab-df-buttons\">\n",
       "\n",
       "  <div class=\"colab-df-container\">\n",
       "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f8fa72ce-c5aa-40bd-ad7d-fed98cee7686')\"\n",
       "            title=\"Convert this dataframe to an interactive table.\"\n",
       "            style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
       "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
       "  </svg>\n",
       "    </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    .colab-df-buttons div {\n",
       "      margin-bottom: 4px;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "    <script>\n",
       "      const buttonEl =\n",
       "        document.querySelector('#df-f8fa72ce-c5aa-40bd-ad7d-fed98cee7686 button.colab-df-convert');\n",
       "      buttonEl.style.display =\n",
       "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "      async function convertToInteractive(key) {\n",
       "        const element = document.querySelector('#df-f8fa72ce-c5aa-40bd-ad7d-fed98cee7686');\n",
       "        const dataTable =\n",
       "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                    [key], {});\n",
       "        if (!dataTable) return;\n",
       "\n",
       "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "          + ' to learn more about interactive tables.';\n",
       "        element.innerHTML = '';\n",
       "        dataTable['output_type'] = 'display_data';\n",
       "        await google.colab.output.renderOutput(dataTable, element);\n",
       "        const docLink = document.createElement('div');\n",
       "        docLink.innerHTML = docLinkHtml;\n",
       "        element.appendChild(docLink);\n",
       "      }\n",
       "    </script>\n",
       "  </div>\n",
       "\n",
       "\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "       SUBJECT_ID             FID\n",
       "0  10011708520314  10011708520314\n",
       "1  10021708520764  10021708520764\n",
       "2  10021708521587  10021708521587"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nulos en survey (top 10 columnas):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SUBJECT_ID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AGE</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RACE</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1D_HIST</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUTO_HIST</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUTO_COND</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AUTO_COND_4_TEXT</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1D_DIAG</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1D_DIAG_AGE</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>T1D_HOSP</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "SUBJECT_ID          0\n",
       "AGE                 0\n",
       "RACE                0\n",
       "T1D_HIST            0\n",
       "AUTO_HIST           0\n",
       "AUTO_COND           0\n",
       "AUTO_COND_4_TEXT    0\n",
       "T1D_DIAG            0\n",
       "T1D_DIAG_AGE        0\n",
       "T1D_HOSP            0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Nulos en genotyping (top 10 columnas):\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>FID</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>contact key</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rs1049225</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rs1052553</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rs10795791</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rs11203203</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rs113010081</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rs1150743</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rs12416116</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rs12720356</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "FID            0\n",
       "contact key    0\n",
       "rs1049225      0\n",
       "rs1052553      0\n",
       "rs10795791     0\n",
       "rs11203203     0\n",
       "rs113010081    0\n",
       "rs1150743      0\n",
       "rs12416116     0\n",
       "rs12720356     0\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total de valores '-9' (string) en genotyping: 181\n"
     ]
    }
   ],
   "source": [
    "# 2. Exploraci√≥n con Pandas (usando CSV convertidos)\n",
    "# Objetivo: validar JOIN real y detectar problemas de calidad (nulos, -9) sobre los CSV.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "survey_csv = \"/content/data/csv/survey.csv\"\n",
    "geno_csv   = \"/content/data/csv/genotyping.csv\"\n",
    "\n",
    "# 1) Cargar CSV forzando texto para evitar problemas de tipos\n",
    "df_s = pd.read_csv(survey_csv, dtype=str)\n",
    "df_g = pd.read_csv(geno_csv, dtype=str)\n",
    "\n",
    "# 2) Confirmaci√≥n de columnas clave\n",
    "assert \"SUBJECT_ID\" in df_s.columns, \"No existe SUBJECT_ID en survey\"\n",
    "assert \"FID\" in df_g.columns, \"No existe FID en genotyping\"\n",
    "\n",
    "# 3) Validaci√≥n JOIN real: survey.SUBJECT_ID = genotyping.FID\n",
    "ids_s = df_s[\"SUBJECT_ID\"].dropna().astype(str).str.strip()\n",
    "ids_g = df_g[\"FID\"].dropna().astype(str).str.strip()\n",
    "\n",
    "print(\"IDs √∫nicos survey (SUBJECT_ID):\", ids_s.nunique())\n",
    "print(\"IDs √∫nicos genotyping (FID):  \", ids_g.nunique())\n",
    "print(\"Intersecci√≥n IDs:\", len(set(ids_s).intersection(set(ids_g))))\n",
    "\n",
    "m = pd.merge(\n",
    "    df_s[[\"SUBJECT_ID\"]],\n",
    "    df_g[[\"FID\"]],\n",
    "    left_on=\"SUBJECT_ID\",\n",
    "    right_on=\"FID\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "print(\"Filas merge inner:\", m.shape[0])\n",
    "display(m.head(3))\n",
    "\n",
    "# 4) Detecci√≥n de nulos / vac√≠os (top 10 columnas)\n",
    "print(\"\\nNulos en survey (top 10 columnas):\")\n",
    "display(df_s.isna().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "print(\"\\nNulos en genotyping (top 10 columnas):\")\n",
    "display(df_g.isna().sum().sort_values(ascending=False).head(10))\n",
    "\n",
    "# 5) Conteo de '-9' como missing codificado en genotyping\n",
    "minus9_total = (df_g == \"-9\").sum().sum()\n",
    "print(\"\\nTotal de valores '-9' (string) en genotyping:\", int(minus9_total))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zdtjMapxOI2T",
   "metadata": {
    "id": "zdtjMapxOI2T"
   },
   "source": [
    "## Conclusiones ( Pandas)\n",
    "- He confirmado que hay **3818 IDs √∫nicos** en ambos archivos.\n",
    "- La intersecci√≥n de identificadores es total (**3818 IDs comunes**) y el `merge inner` devuelve **3818 filas**, validando la clave de uni√≥n **`survey.SUBJECT_ID = genotyping.FID`**.\n",
    "- En el top 10 de columnas revisadas no he detectado valores nulos (`NaN`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VmrGZiCrzxpd",
   "metadata": {
    "id": "VmrGZiCrzxpd"
   },
   "source": [
    "## Localizar exactamente d√≥nde est√° el missing codificado (-9)\n",
    "\n",
    "\n",
    "> Objetivo: identificar qu√© columnas concretas contienen '-9' para justificar la limpieza en Pig.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "_N4TZAQ4F2Ay",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 29,
     "status": "ok",
     "timestamp": 1771447224053,
     "user": {
      "displayName": "Alfredo Ledesma Ruiz",
      "userId": "02263706330541384863"
     },
     "user_tz": -60
    },
    "id": "_N4TZAQ4F2Ay",
    "outputId": "a73b326e-2a20-4559-b667-89e3c36a208d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columnas con '-9' (missing codificado) en genotyping:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rs9585056</th>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rs12927355</th>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rs1367728</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rs72727394</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "rs9585056     89\n",
       "rs12927355    82\n",
       "rs1367728      9\n",
       "rs72727394     1\n",
       "dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total '-9' en genotyping: 181\n",
      "\n",
      "Columnas con cadenas vac√≠as '' en survey:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div><br><label><b>dtype:</b> int64</label>"
      ],
      "text/plain": [
       "Series([], dtype: int64)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Localizar exactamente d√≥nde est√° el missing codificado (-9)\n",
    "# Objetivo: identificar qu√© columnas concretas contienen '-9' para justificar la limpieza en Pig.\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "survey_csv = \"/content/data/csv/survey.csv\"\n",
    "geno_csv   = \"/content/data/csv/genotyping.csv\"\n",
    "\n",
    "df_s = pd.read_csv(survey_csv, dtype=str)\n",
    "df_g = pd.read_csv(geno_csv, dtype=str)\n",
    "\n",
    "# 1) Conteo por columna de '-9' en genotyping (missing codificado)\n",
    "minus9_by_column = (df_g == \"-9\").sum()\n",
    "minus9_by_column = minus9_by_column[minus9_by_column > 0].sort_values(ascending=False)\n",
    "\n",
    "print(\"Columnas con '-9' (missing codificado) en genotyping:\")\n",
    "display(minus9_by_column)\n",
    "\n",
    "print(\"\\nTotal '-9' en genotyping:\", int((df_g == \"-9\").sum().sum()))\n",
    "\n",
    "# 2) Comprobaci√≥n de cadenas vac√≠as en survey (por si hubiese valores \"\" en vez de NaN)\n",
    "empty_strings_survey = (df_s == \"\").sum()\n",
    "empty_strings_survey = empty_strings_survey[empty_strings_survey > 0]\n",
    "\n",
    "print(\"\\nColumnas con cadenas vac√≠as '' en survey:\")\n",
    "display(empty_strings_survey)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ol67aeHXOREk",
   "metadata": {
    "id": "Ol67aeHXOREk"
   },
   "source": [
    "## Conclusiones (calidad de datos)\n",
    "- He detectado un total de **181 valores `\"-9\"`** en la tabla genotyping (missing codificado).\n",
    "- El desglose es: `rs9585056` (89), `rs12927355` (82), `rs1367728` (9) y `rs72727394` (1).\n",
    "- He comprobado que no existen cadenas vac√≠as (`\"\"`) en la tabla survey.\n",
    "- En el siguiente apartado limpiar√© estos `\"-9\"` convirti√©ndolos a valores nulos reales para no distorsionar transformaciones y consultas.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0decad02",
   "metadata": {
    "id": "0decad02"
   },
   "source": [
    "# 3) Apache Pig ‚Äî Limpieza + tratamiento\n",
    "En esta secci√≥n voy a preparar Pig para limpiar datos de genotyping y normalizar claves para asegurar un JOIN fiable.\n",
    "Primero dejar√© el entorno listo en Colab con Java 17 y Pig, siguiendo el estilo de los cuadernos de referencia.\n",
    "Como tratamiento interesante simple, usar√© un Top 3 de valores m√°s frecuentes en una columna clave tras la limpieza."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b23bd4e2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 140038,
     "status": "ok",
     "timestamp": 1771447364094,
     "user": {
      "displayName": "Alfredo Ledesma Ruiz",
      "userId": "02263706330541384863"
     },
     "user_tz": -60
    },
    "id": "b23bd4e2",
    "outputId": "bee561c3-7a85-4710-c7a0-d4e6cd011d72"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.17\" 2025-10-21\n",
      "OpenJDK Runtime Environment (build 17.0.17+10-Ubuntu-122.04)\n",
      "OpenJDK 64-Bit Server VM (build 17.0.17+10-Ubuntu-122.04, mixed mode, sharing)\n",
      "Hadoop 3.4.2\n",
      "Source code repository https://github.com/apache/hadoop.git -r 84e8b89ee2ebe6923691205b9e171badde7a495c\n",
      "Compiled by ahmarsu on 2025-08-20T10:30Z\n",
      "Apache Pig version 0.17.0 (r1797386) \n",
      "compiled Jun 02 2017, 15:41:58\n"
     ]
    }
   ],
   "source": [
    "# Secci√≥n 3.1 ‚Äî Preparaci√≥n de entorno Pig en Colab (Java 17)\n",
    "# Estilo base: cuaderno 0301 (instalaci√≥n simple y verificaci√≥n de versi√≥n)\n",
    "\n",
    "import os\n",
    "\n",
    "# 1) Java 17\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "\n",
    "# 2) Hadoop (dependencia de entorno para Pig)\n",
    "!wget -q https://downloads.apache.org/hadoop/common/hadoop-3.4.2/hadoop-3.4.2.tar.gz\n",
    "!tar -xzf hadoop-3.4.2.tar.gz\n",
    "!rm -rf /usr/local/hadoop\n",
    "!mv hadoop-3.4.2 /usr/local/hadoop\n",
    "\n",
    "os.environ[\"HADOOP_HOME\"] = \"/usr/local/hadoop\"\n",
    "os.environ[\"PATH\"] += f\":{os.environ['HADOOP_HOME']}/bin:{os.environ['HADOOP_HOME']}/sbin\"\n",
    "\n",
    "# 3) Pig\n",
    "!wget -q https://downloads.apache.org/pig/pig-0.17.0/pig-0.17.0.tar.gz\n",
    "!tar -xzf pig-0.17.0.tar.gz\n",
    "!rm -rf /usr/local/pig-0.17.0\n",
    "!mv pig-0.17.0 /usr/local/pig-0.17.0\n",
    "\n",
    "os.environ[\"PIG_HOME\"] = \"/usr/local/pig-0.17.0\"\n",
    "os.environ[\"PATH\"] += f\":{os.environ['PIG_HOME']}/bin\"\n",
    "os.environ[\"PIG_CLASSPATH\"] = \"/usr/local/hadoop/etc/hadoop\"\n",
    "\n",
    "# 4) Verificaci√≥n m√≠nima visible\n",
    "!java -version\n",
    "!hadoop version | head -n 3\n",
    "!pig -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e70955a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 24,
     "status": "ok",
     "timestamp": 1771447364140,
     "user": {
      "displayName": "Alfredo Ledesma Ruiz",
      "userId": "02263706330541384863"
     },
     "user_tz": -60
    },
    "id": "4e70955a",
    "outputId": "7c763a68-4b5e-4057-bd3c-49436fa4d21a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing limpieza.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile limpieza.pig\n",
    "-- Secci√≥n 3.1: limpieza de survey y genotyping\n",
    "\n",
    "survey_raw = LOAD '/content/data/csv/survey.csv' USING PigStorage(',') AS (\n",
    "    SUBJECT_ID:chararray,\n",
    "    AGE:chararray,\n",
    "    RACE:chararray,\n",
    "    T1D_HIST:chararray,\n",
    "    AUTO_HIST:chararray,\n",
    "    AUTO_COND:chararray,\n",
    "    AUTO_COND_4_TEXT:chararray,\n",
    "    T1D_DIAG:chararray,\n",
    "    T1D_DIAG_AGE:chararray,\n",
    "    T1D_HOSP:chararray,\n",
    "    DKA:chararray,\n",
    "    GRS_HLA:chararray,\n",
    "    GnonHLA:chararray,\n",
    "    GRS:chararray,\n",
    "    Risk:chararray\n",
    ");\n",
    "\n",
    "survey_no_header = FILTER survey_raw BY SUBJECT_ID != 'SUBJECT_ID';\n",
    "\n",
    "survey_clean = FOREACH survey_no_header GENERATE\n",
    "    TRIM(SUBJECT_ID) AS SUBJECT_ID,\n",
    "    AGE,\n",
    "    RACE,\n",
    "    T1D_HIST,\n",
    "    AUTO_HIST,\n",
    "    AUTO_COND,\n",
    "    AUTO_COND_4_TEXT,\n",
    "    T1D_DIAG,\n",
    "    T1D_DIAG_AGE,\n",
    "    T1D_HOSP,\n",
    "    DKA,\n",
    "    GRS_HLA,\n",
    "    GnonHLA,\n",
    "    GRS,\n",
    "    Risk;\n",
    "\n",
    "STORE survey_clean INTO '/content/data/pig_out/survey_clean' USING PigStorage(',');\n",
    "\n",
    "\n",
    "genotyping_raw = LOAD '/content/data/csv/genotyping.csv' USING PigStorage(',') AS (\n",
    "    FID:chararray,\n",
    "    contact_key:chararray,\n",
    "    rs1049225:chararray,\n",
    "    rs1052553:chararray,\n",
    "    rs10795791:chararray,\n",
    "    rs11203203:chararray,\n",
    "    rs113010081:chararray,\n",
    "    rs1150743:chararray,\n",
    "    rs12416116:chararray,\n",
    "    rs12720356:chararray,\n",
    "    rs12927355:chararray,\n",
    "    rs12971201:chararray,\n",
    "    rs13415583:chararray,\n",
    "    rs1367728:chararray,\n",
    "    rs1456988:chararray,\n",
    "    rs151233:chararray,\n",
    "    rs1574285:chararray,\n",
    "    rs1615504:chararray,\n",
    "    rs1893217:chararray,\n",
    "    rs193778:chararray,\n",
    "    rs2045258:chararray,\n",
    "    rs2071463:chararray,\n",
    "    rs2076531:chararray,\n",
    "    rs2111485:chararray,\n",
    "    rs2143461:chararray,\n",
    "    rs2194225:chararray,\n",
    "    rs2239800:chararray,\n",
    "    rs2256974:chararray,\n",
    "    rs229533:chararray,\n",
    "    rs2476601:chararray,\n",
    "    rs2523409:chararray,\n",
    "    rs2524089:chararray,\n",
    "    rs2611215:chararray,\n",
    "    rs28732101:chararray,\n",
    "    rs3024505:chararray,\n",
    "    rs3087243:chararray,\n",
    "    rs3094165:chararray,\n",
    "    rs3129722:chararray,\n",
    "    rs3130933:chararray,\n",
    "    rs34536443:chararray,\n",
    "    rs34593439:chararray,\n",
    "    rs35337543:chararray,\n",
    "    rs35667974:chararray,\n",
    "    rs3763305:chararray,\n",
    "    rs402072:chararray,\n",
    "    rs41295121:chararray,\n",
    "    rs436845:chararray,\n",
    "    rs4820830:chararray,\n",
    "    rs4849135:chararray,\n",
    "    rs516246:chararray,\n",
    "    rs56994090:chararray,\n",
    "    rs6043409:chararray,\n",
    "    rs61839660:chararray,\n",
    "    rs62447205:chararray,\n",
    "    rs635688:chararray,\n",
    "    rs6518350:chararray,\n",
    "    rs653178:chararray,\n",
    "    rs6691977:chararray,\n",
    "    rs689:chararray,\n",
    "    rs6903608:chararray,\n",
    "    rs6906897:chararray,\n",
    "    rs6935715:chararray,\n",
    "    rs705704:chararray,\n",
    "    rs72727394:chararray,\n",
    "    rs72853903:chararray,\n",
    "    rs72928038:chararray,\n",
    "    rs757411:chararray,\n",
    "    rs7745656:chararray,\n",
    "    rs7780389:chararray,\n",
    "    rs917911:chararray,\n",
    "    rs9268633:chararray,\n",
    "    rs9271366:chararray,\n",
    "    rs9273363:chararray,\n",
    "    rs9357152:chararray,\n",
    "    rs9469341:chararray,\n",
    "    rs9585056:chararray\n",
    ");\n",
    "\n",
    "genotyping_no_header = FILTER genotyping_raw BY FID != 'FID';\n",
    "\n",
    "genotyping_clean = FOREACH genotyping_no_header GENERATE\n",
    "    TRIM(FID) AS FID,\n",
    "    contact_key,\n",
    "    rs1049225,\n",
    "    rs1052553,\n",
    "    rs10795791,\n",
    "    rs11203203,\n",
    "    rs113010081,\n",
    "    rs1150743,\n",
    "    rs12416116,\n",
    "    rs12720356,\n",
    "    (rs12927355 == '-9' ? '' : rs12927355) AS rs12927355,\n",
    "    rs12971201,\n",
    "    rs13415583,\n",
    "    (rs1367728 == '-9' ? '' : rs1367728) AS rs1367728,\n",
    "    rs1456988,\n",
    "    rs151233,\n",
    "    rs1574285,\n",
    "    rs1615504,\n",
    "    rs1893217,\n",
    "    rs193778,\n",
    "    rs2045258,\n",
    "    rs2071463,\n",
    "    rs2076531,\n",
    "    rs2111485,\n",
    "    rs2143461,\n",
    "    rs2194225,\n",
    "    rs2239800,\n",
    "    rs2256974,\n",
    "    rs229533,\n",
    "    rs2476601,\n",
    "    rs2523409,\n",
    "    rs2524089,\n",
    "    rs2611215,\n",
    "    rs28732101,\n",
    "    rs3024505,\n",
    "    rs3087243,\n",
    "    rs3094165,\n",
    "    rs3129722,\n",
    "    rs3130933,\n",
    "    rs34536443,\n",
    "    rs34593439,\n",
    "    rs35337543,\n",
    "    rs35667974,\n",
    "    rs3763305,\n",
    "    rs402072,\n",
    "    rs41295121,\n",
    "    rs436845,\n",
    "    rs4820830,\n",
    "    rs4849135,\n",
    "    rs516246,\n",
    "    rs56994090,\n",
    "    rs6043409,\n",
    "    rs61839660,\n",
    "    rs62447205,\n",
    "    rs635688,\n",
    "    rs6518350,\n",
    "    rs653178,\n",
    "    rs6691977,\n",
    "    rs689,\n",
    "    rs6903608,\n",
    "    rs6906897,\n",
    "    rs6935715,\n",
    "    rs705704,\n",
    "    (rs72727394 == '-9' ? '' : rs72727394) AS rs72727394,\n",
    "    rs72853903,\n",
    "    rs72928038,\n",
    "    rs757411,\n",
    "    rs7745656,\n",
    "    rs7780389,\n",
    "    rs917911,\n",
    "    rs9268633,\n",
    "    rs9271366,\n",
    "    rs9273363,\n",
    "    rs9357152,\n",
    "    rs9469341,\n",
    "    (rs9585056 == '-9' ? '' : rs9585056) AS rs9585056;\n",
    "\n",
    "STORE genotyping_clean INTO '/content/data/pig_out/genotyping_clean' USING PigStorage(',');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "df083b23",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 10659,
     "status": "ok",
     "timestamp": 1771447374805,
     "user": {
      "displayName": "Alfredo Ledesma Ruiz",
      "userId": "02263706330541384863"
     },
     "user_tz": -60
    },
    "id": "df083b23",
    "outputId": "78bd2bc6-5a9b-4cb2-f5fe-142fc45d4252"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-18 20:42:46,308 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL\n",
      "2026-02-18 20:42:46,309 INFO pig.ExecTypeProvider: Picked LOCAL as the ExecType\n",
      "2026-02-18 20:42:46,477 [main] INFO  org.apache.pig.Main - Apache Pig version 0.17.0 (r1797386) compiled Jun 02 2017, 15:41:58\n",
      "2026-02-18 20:42:46,477 [main] INFO  org.apache.pig.Main - Logging error messages to: /content/pig_1771447366467.log\n",
      "2026-02-18 20:42:46,550 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2026-02-18 20:42:47,124 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\n",
      "2026-02-18 20:42:47,472 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2026-02-18 20:42:47,480 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///\n",
      "2026-02-18 20:42:47,583 [main] INFO  org.apache.pig.PigServer - Pig Script ID for the session: PIG-limpieza.pig-fd9b3741-0b9b-4004-ac5d-223627b487ba\n",
      "2026-02-18 20:42:47,584 [main] WARN  org.apache.pig.PigServer - ATS is disabled since yarn.timeline-service.enabled set to false\n",
      "2026-02-18 20:42:49,035 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2026-02-18 20:42:49,110 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: FILTER\n",
      "2026-02-18 20:42:49,324 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NestedLimitOptimizer, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}\n",
      "2026-02-18 20:42:49,574 [main] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n",
      "2026-02-18 20:42:49,665 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\n",
      "2026-02-18 20:42:49,708 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 2\n",
      "2026-02-18 20:42:49,709 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 2\n",
      "2026-02-18 20:42:50,212 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.MRScriptState - Pig script settings are added to the job\n",
      "2026-02-18 20:42:50,222 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2026-02-18 20:42:50,222 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2026-02-18 20:42:50,227 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2026-02-18 20:42:50,264 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2026-02-18 20:42:50,279 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2026-02-18 20:42:50,280 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cacche\n",
      "2026-02-18 20:42:50,280 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /tmp/1771447370279-0\n",
      "2026-02-18 20:42:50,403 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.MRScriptState - Pig script settings are added to the job\n",
      "2026-02-18 20:42:50,406 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2026-02-18 20:42:50,413 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2026-02-18 20:42:50,417 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2026-02-18 20:42:50,417 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cacche\n",
      "2026-02-18 20:42:50,417 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /tmp/1771447370417-0\n",
      "2026-02-18 20:42:50,516 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 2 map-reduce job(s) waiting for submission.\n",
      "2026-02-18 20:42:50,533 [JobControl] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:50,563 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2026-02-18 20:42:50,735 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2026-02-18 20:42:50,748 [JobControl] INFO  org.apache.pig.builtin.PigStorage - Using PigTextInputFormat\n",
      "2026-02-18 20:42:50,760 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2026-02-18 20:42:50,760 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2026-02-18 20:42:50,825 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
      "2026-02-18 20:42:50,945 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2026-02-18 20:42:51,393 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local164275374_0001\n",
      "2026-02-18 20:42:51,396 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []\n",
      "2026-02-18 20:42:51,664 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n",
      "2026-02-18 20:42:51,664 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n",
      "2026-02-18 20:42:51,666 [JobControl] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:51,694 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2026-02-18 20:42:51,696 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2026-02-18 20:42:51,698 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2026-02-18 20:42:51,706 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2026-02-18 20:42:51,715 [JobControl] INFO  org.apache.pig.builtin.PigStorage - Using PigTextInputFormat\n",
      "2026-02-18 20:42:51,721 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2026-02-18 20:42:51,721 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2026-02-18 20:42:51,722 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
      "2026-02-18 20:42:51,738 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2026-02-18 20:42:51,743 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:42:51,746 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:42:51,746 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:42:51,747 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n",
      "2026-02-18 20:42:51,786 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1100808292_0002\n",
      "2026-02-18 20:42:51,786 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []\n",
      "2026-02-18 20:42:51,874 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n",
      "2026-02-18 20:42:51,875 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local164275374_0001_m_000000_0\n",
      "2026-02-18 20:42:52,027 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.textoutputformat.separator is deprecated. Instead, use mapreduce.output.textoutputformat.separator\n",
      "2026-02-18 20:42:52,032 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:42:52,032 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:42:52,032 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:42:52,084 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n",
      "2026-02-18 20:42:52,112 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2026-02-18 20:42:52,113 [Thread-6] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n",
      "2026-02-18 20:42:52,137 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 539738\n",
      "Input split[0]:\n",
      "   Length = 539738\n",
      "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
      "   Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2026-02-18 20:42:52,161 [Thread-6] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2026-02-18 20:42:52,162 [Thread-6] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2026-02-18 20:42:52,163 [Thread-6] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:42:52,163 [Thread-6] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:42:52,163 [Thread-6] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:42:52,163 [Thread-6] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n",
      "2026-02-18 20:42:52,181 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.builtin.PigStorage - Using PigTextInputFormat\n",
      "2026-02-18 20:42:52,185 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/data/csv/survey.csv:0+539738\n",
      "2026-02-18 20:42:52,195 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:42:52,195 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:42:52,195 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:42:52,199 [Thread-6] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n",
      "2026-02-18 20:42:52,199 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1100808292_0002_m_000000_0\n",
      "2026-02-18 20:42:52,227 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n",
      "2026-02-18 20:42:52,229 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.\n",
      "2026-02-18 20:42:52,245 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:42:52,247 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:42:52,247 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:42:52,262 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2026-02-18 20:42:52,266 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map - Aliases being processed per job phase (AliasName[line,offset]): M: survey_raw[3,13],survey_raw[-1,-1],survey_no_header[21,19],survey_clean[23,15] C:  R: \n",
      "2026-02-18 20:42:52,270 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 1243581\n",
      "Input split[0]:\n",
      "   Length = 1243581\n",
      "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
      "   Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2026-02-18 20:42:52,282 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.builtin.PigStorage - Using PigTextInputFormat\n",
      "2026-02-18 20:42:52,283 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/data/csv/genotyping.csv:0+1243581\n",
      "2026-02-18 20:42:52,299 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:42:52,299 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:42:52,299 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:42:52,320 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n",
      "2026-02-18 20:42:52,322 [LocalJobRunner Map Task Executor #0] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2026-02-18 20:42:52,443 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map - Aliases being processed per job phase (AliasName[line,offset]): M: genotyping_raw[43,17],genotyping_raw[-1,-1],genotyping_no_header[122,23],genotyping_clean[124,19] C:  R: \n",
      "2026-02-18 20:42:52,586 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local164275374_0001\n",
      "2026-02-18 20:42:52,587 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases survey_clean,survey_no_header,survey_raw\n",
      "2026-02-18 20:42:52,587 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: survey_raw[3,13],survey_raw[-1,-1],survey_no_header[21,19],survey_clean[23,15] C:  R: \n",
      "2026-02-18 20:42:52,599 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local1100808292_0002\n",
      "2026-02-18 20:42:52,599 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases genotyping_clean,genotyping_no_header,genotyping_raw\n",
      "2026-02-18 20:42:52,599 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: genotyping_raw[43,17],genotyping_raw[-1,-1],genotyping_no_header[122,23],genotyping_clean[124,19] C:  R: \n",
      "2026-02-18 20:42:52,623 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n",
      "2026-02-18 20:42:52,623 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_local164275374_0001,job_local1100808292_0002]\n",
      "2026-02-18 20:42:52,896 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - \n",
      "2026-02-18 20:42:52,919 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local164275374_0001_m_000000_0 is done. And is in the process of committing\n",
      "2026-02-18 20:42:52,927 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - \n",
      "2026-02-18 20:42:52,928 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task attempt_local164275374_0001_m_000000_0 is allowed to commit now\n",
      "2026-02-18 20:42:52,936 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local164275374_0001_m_000000_0' to file:/content/data/pig_out/survey_clean\n",
      "2026-02-18 20:42:52,937 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map\n",
      "2026-02-18 20:42:52,938 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local164275374_0001_m_000000_0' done.\n",
      "2026-02-18 20:42:52,947 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local164275374_0001_m_000000_0: Counters: 15\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=782240\n",
      "\t\tFILE: Number of bytes written=2143452\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3819\n",
      "\t\tMap output records=3818\n",
      "\t\tInput split bytes=360\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=45\n",
      "\t\tTotal committed heap usage (bytes)=125829120\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "2026-02-18 20:42:52,948 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local164275374_0001_m_000000_0\n",
      "2026-02-18 20:42:52,950 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n",
      "2026-02-18 20:42:53,125 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete\n",
      "2026-02-18 20:42:53,126 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_local1100808292_0002]\n",
      "2026-02-18 20:42:53,138 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:53,177 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:53,181 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2026-02-18 20:42:53,181 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2026-02-18 20:42:53,187 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:53,466 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - \n",
      "2026-02-18 20:42:53,468 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1100808292_0002_m_000000_0 is done. And is in the process of committing\n",
      "2026-02-18 20:42:53,479 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - \n",
      "2026-02-18 20:42:53,479 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task attempt_local1100808292_0002_m_000000_0 is allowed to commit now\n",
      "2026-02-18 20:42:53,492 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local1100808292_0002_m_000000_0' to file:/content/data/pig_out/genotyping_clean\n",
      "2026-02-18 20:42:53,494 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map\n",
      "2026-02-18 20:42:53,496 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1100808292_0002_m_000000_0' done.\n",
      "2026-02-18 20:42:53,497 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1100808292_0002_m_000000_0: Counters: 15\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1784157\n",
      "\t\tFILE: Number of bytes written=3158781\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3819\n",
      "\t\tMap output records=3818\n",
      "\t\tInput split bytes=364\n",
      "\t\tSpilled Records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=17\n",
      "\t\tTotal committed heap usage (bytes)=125829120\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "2026-02-18 20:42:53,498 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1100808292_0002_m_000000_0\n",
      "2026-02-18 20:42:53,500 [Thread-6] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n",
      "2026-02-18 20:42:53,634 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:53,641 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:53,646 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:53,654 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n",
      "2026-02-18 20:42:53,664 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.SimplePigStats - Script Statistics: \n",
      "\n",
      "HadoopVersion\tPigVersion\tUserId\tStartedAt\tFinishedAt\tFeatures\n",
      "3.4.2\t0.17.0\troot\t2026-02-18 20:42:50\t2026-02-18 20:42:53\tFILTER\n",
      "\n",
      "Success!\n",
      "\n",
      "Job Stats (time in seconds):\n",
      "JobId\tMaps\tReduces\tMaxMapTime\tMinMapTime\tAvgMapTime\tMedianMapTime\tMaxReduceTime\tMinReduceTime\tAvgReduceTime\tMedianReducetime\tAlias\tFeature\tOutputs\n",
      "job_local1100808292_0002\t1\t0\tn/a\tn/a\tn/a\tn/a\t0\t0\t0\t0\tgenotyping_clean,genotyping_no_header,genotyping_raw\tMAP_ONLY\t/content/data/pig_out/genotyping_clean,\n",
      "job_local164275374_0001\t1\t0\tn/a\tn/a\tn/a\tn/a\t0\t0\t0\t0\tsurvey_clean,survey_no_header,survey_raw\tMAP_ONLY\t/content/data/pig_out/survey_clean,\n",
      "\n",
      "Input(s):\n",
      "Successfully read 3819 records from: \"/content/data/csv/survey.csv\"\n",
      "Successfully read 3819 records from: \"/content/data/csv/genotyping.csv\"\n",
      "\n",
      "Output(s):\n",
      "Successfully stored 3818 records in: \"/content/data/pig_out/survey_clean\"\n",
      "Successfully stored 3818 records in: \"/content/data/pig_out/genotyping_clean\"\n",
      "\n",
      "Counters:\n",
      "Total records written : 7636\n",
      "Total bytes written : 0\n",
      "Spillable Memory Manager spill count : 0\n",
      "Total bags proactively spilled: 0\n",
      "Total records proactively spilled: 0\n",
      "\n",
      "Job DAG:\n",
      "job_local164275374_0001\n",
      "job_local1100808292_0002\n",
      "\n",
      "\n",
      "2026-02-18 20:42:53,669 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:53,676 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:53,688 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:53,715 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:53,721 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:53,726 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:53,736 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Success!\n",
      "2026-02-18 20:42:53,768 [main] INFO  org.apache.pig.Main - Pig script completed in 7 seconds and 880 milliseconds (7880 ms)\n",
      "\n",
      "Salida survey_clean:\n",
      "total 524K\n",
      "-rw-r--r-- 1 root root 523K Feb 18 20:42 part-m-00000\n",
      "-rw-r--r-- 1 root root    0 Feb 18 20:42 _SUCCESS\n",
      "Primeras 3 filas survey_clean:\n",
      "10011708520314,6,White,Yes,No,Not applicable,Not applicable,No,Not applicable,Not applicable,Not applicable,1.91,0.14,2.06,Not high\n",
      "10021708520764,3,White,Don't know,Yes,\"Thyroid_Hashimotos and_or Graves, Blood relative has been diagnosed with autoimmune disease but I don't know which condition\",Not applicable,No,Not applicable,Not applicable,Not applicable,-1.41,1.93,0.52\n",
      "10021708521587,7,Asian,Don't know,No,Not applicable,Not applicable,No,Not applicable,Not applicable,Not applicable,-13.66,0.77,-12.89,Not high\n",
      "\n",
      "Salida genotyping_clean:\n",
      "total 1.2M\n",
      "-rw-r--r-- 1 root root 1.2M Feb 18 20:42 part-m-00000\n",
      "-rw-r--r-- 1 root root    0 Feb 18 20:42 _SUCCESS\n",
      "Primeras 3 filas genotyping_clean:\n",
      "10011708520314,CONTACT10085,C:C,A:A,A:G,A:G,T:T,A:G,C:C,T:T,T:C,A:G,G:T,A:G,G:T,A:G,G:G,G:G,T:T,T:T,C:T,G:G,T:T,A:G,C:C,A:A,C:T,T:G,A:A,G:G,C:T,A:A,C:C,C:C,C:C,A:G,C:T,C:C,G:G,G:G,G:G,G:G,A:A,A:G,A:G,C:T,C:T,T:C,G:G,A:G,T:T,A:G,C:C,A:A,C:T,A:A,G:G,T:T,A:A,T:T,C:C,T:T,G:G,C:T,C:T,G:G,C:T,G:T,C:C,G:T,A:G,A:A,A:C,A:A,A:G,T:T\n",
      "10021708520764,CONTACT14053,C:C,A:A,G:G,A:A,T:T,G:G,C:C,T:T,C:C,G:G,T:T,G:G,G:T,G:G,G:T,A:G,C:T,C:T,C:C,A:G,T:T,G:G,C:C,A:G,T:T,G:G,C:C,A:G,C:C,A:C,C:C,C:C,C:T,A:G,C:C,C:C,G:G,G:G,G:G,G:G,A:A,G:G,A:A,C:C,T:T,T:T,G:G,G:G,T:T,A:A,C:C,A:G,T:T,A:A,A:G,T:T,A:T,C:T,C:C,T:T,A:G,C:C,C:C,G:G,C:C,T:T,C:C,G:T,G:G,A:A,C:C,A:G,A:G,C:T\n",
      "10021708521587,CONTACT11350,C:C,A:G,A:G,G:G,T:T,A:A,A:A,T:T,T:T,G:G,T:T,G:G,G:G,A:A,G:T,A:G,T:T,T:T,C:C,A:A,C:C,A:G,C:C,A:A,T:T,G:G,A:C,G:G,T:T,A:A,C:C,C:C,C:C,A:A,T:T,C:C,G:G,G:G,G:G,G:G,A:A,G:G,A:A,C:C,T:T,T:T,T:T,G:G,T:T,A:G,C:C,A:A,C:C,A:A,A:G,C:T,A:A,T:T,C:C,T:T,A:G,C:T,C:C,A:G,C:T,T:T,C:C,T:T,A:A,G:G,C:C,A:A,G:G,C:T\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar limpieza Pig en modo local y mostrar evidencias m√≠nimas\n",
    "\n",
    "!rm -rf /content/data/pig_out/survey_clean /content/data/pig_out/genotyping_clean\n",
    "!pig -x local -f limpieza.pig\n",
    "\n",
    "print(\"\\nSalida survey_clean:\")\n",
    "!ls -lh /content/data/pig_out/survey_clean\n",
    "print(\"Primeras 3 filas survey_clean:\")\n",
    "!head -n 3 /content/data/pig_out/survey_clean/part*\n",
    "\n",
    "print(\"\\nSalida genotyping_clean:\")\n",
    "!ls -lh /content/data/pig_out/genotyping_clean\n",
    "print(\"Primeras 3 filas genotyping_clean:\")\n",
    "!head -n 3 /content/data/pig_out/genotyping_clean/part*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38a71f32",
   "metadata": {
    "id": "38a71f32"
   },
   "source": [
    "## Conclusiones (Secci√≥n 3.1 ‚Äî Limpieza con Pig)‚Äù\n",
    "\n",
    "He ejecutado limpieza.pig en modo local y la limpieza ha funcionado correctamente. Se han generado las carpetas de salida survey_clean y genotyping_clean dentro de /content/data/pig_out/, cada una con su fichero part-m-00000 y el marcador _SUCCESS. En concreto, survey_clean ocupa aproximadamente 523 KB y genotyping_clean aproximadamente 1.2 MB.\n",
    "\n",
    "\n",
    "Adem√°s, he normalizado las claves de relaci√≥n aplicando TRIM() sobre SUBJECT_ID y FID, dejando los datos preparados para un JOIN posterior fiable. Por √∫ltimo, he aplicado la correcci√≥n del missing codificado sustituyendo '-9' por vac√≠o ('') √∫nicamente en los 4 marcadores identificados previamente (rs9585056, rs12927355, rs1367728 y rs72727394), manteniendo el resto de columnas sin cambios."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd39c2f9",
   "metadata": {
    "id": "bd39c2f9"
   },
   "source": [
    "## Secci√≥n 3.2 ‚Äî Tratamiento interesante\n",
    "En este bloque calculo el Top 3 de valores m√°s frecuentes de `RACE` a partir de `survey_clean` con Pig, mostrando el resultado en pantalla con `DUMP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eb48c14a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 58,
     "status": "ok",
     "timestamp": 1771447374867,
     "user": {
      "displayName": "Alfredo Ledesma Ruiz",
      "userId": "02263706330541384863"
     },
     "user_tz": -60
    },
    "id": "eb48c14a",
    "outputId": "df0b26f7-069d-4e49-d706-ca51279fdd58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing tratamiento_top3_race.pig\n"
     ]
    }
   ],
   "source": [
    "%%writefile tratamiento_top3_race.pig\n",
    "-- Secci√≥n 3.2: tratamiento interesante simple (Top 3 de RACE)\n",
    "\n",
    "survey_clean = LOAD '/content/data/pig_out/survey_clean' USING PigStorage(',') AS (\n",
    "    SUBJECT_ID:chararray,\n",
    "    AGE:chararray,\n",
    "    RACE:chararray,\n",
    "    T1D_HIST:chararray,\n",
    "    AUTO_HIST:chararray,\n",
    "    AUTO_COND:chararray,\n",
    "    AUTO_COND_4_TEXT:chararray,\n",
    "    T1D_DIAG:chararray,\n",
    "    T1D_DIAG_AGE:chararray,\n",
    "    T1D_HOSP:chararray,\n",
    "    DKA:chararray,\n",
    "    GRS_HLA:chararray,\n",
    "    GnonHLA:chararray,\n",
    "    GRS:chararray,\n",
    "    Risk:chararray\n",
    ");\n",
    "\n",
    "survey_valid = FILTER survey_clean BY (RACE IS NOT NULL) AND (TRIM(RACE) != '');\n",
    "\n",
    "race_group = GROUP survey_valid BY RACE;\n",
    "race_count = FOREACH race_group GENERATE group AS RACE, COUNT(survey_valid) AS total;\n",
    "race_order = ORDER race_count BY $1 DESC;\n",
    "top3_race = LIMIT race_order 3;\n",
    "\n",
    "DUMP top3_race;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "41d2e791",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "executionInfo": {
     "elapsed": 10793,
     "status": "ok",
     "timestamp": 1771447385651,
     "user": {
      "displayName": "Alfredo Ledesma Ruiz",
      "userId": "02263706330541384863"
     },
     "user_tz": -60
    },
    "id": "41d2e791",
    "outputId": "e866c5aa-a3af-4dd8-c4d7-4dc408fcb960"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-02-18 20:42:56,723 INFO pig.ExecTypeProvider: Trying ExecType : LOCAL\n",
      "2026-02-18 20:42:56,723 INFO pig.ExecTypeProvider: Picked LOCAL as the ExecType\n",
      "2026-02-18 20:42:56,841 [main] INFO  org.apache.pig.Main - Apache Pig version 0.17.0 (r1797386) compiled Jun 02 2017, 15:41:58\n",
      "2026-02-18 20:42:56,841 [main] INFO  org.apache.pig.Main - Logging error messages to: /content/pig_1771447376833.log\n",
      "2026-02-18 20:42:56,895 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - user.name is deprecated. Instead, use mapreduce.job.user.name\n",
      "2026-02-18 20:42:57,191 [main] INFO  org.apache.pig.impl.util.Utils - Default bootup file /root/.pigbootup not found\n",
      "2026-02-18 20:42:57,319 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2026-02-18 20:42:57,324 [main] INFO  org.apache.pig.backend.hadoop.executionengine.HExecutionEngine - Connecting to hadoop file system at: file:///\n",
      "2026-02-18 20:42:57,370 [main] INFO  org.apache.pig.PigServer - Pig Script ID for the session: PIG-tratamiento_top3_race.pig-69db64d4-652a-430a-aea5-f6d9a09e6a1a\n",
      "2026-02-18 20:42:57,370 [main] WARN  org.apache.pig.PigServer - ATS is disabled since yarn.timeline-service.enabled set to false\n",
      "2026-02-18 20:42:58,164 [main] INFO  org.apache.pig.tools.pigstats.ScriptState - Pig features used in the script: GROUP_BY,ORDER_BY,FILTER,LIMIT\n",
      "2026-02-18 20:42:58,261 [main] INFO  org.apache.pig.newplan.logical.optimizer.LogicalPlanOptimizer - {RULES_ENABLED=[AddForEach, ColumnMapKeyPrune, ConstantCalculator, GroupByConstParallelSetter, LimitOptimizer, LoadTypeCastInserter, MergeFilter, MergeForEach, NestedLimitOptimizer, PartitionFilterOptimizer, PredicatePushdownOptimizer, PushDownForEachFlatten, PushUpFilter, SplitFilter, StreamTypeCastInserter]}\n",
      "2026-02-18 20:42:58,378 [main] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n",
      "2026-02-18 20:42:58,447 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MRCompiler - File concatenation threshold: 100 optimistic? false\n",
      "2026-02-18 20:42:58,475 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.CombinerOptimizerUtil - Choosing to move algebraic foreach to combiner\n",
      "2026-02-18 20:42:58,500 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.SecondaryKeyOptimizerMR - Using Secondary Key Optimization for MapReduce node scope-72\n",
      "2026-02-18 20:42:58,511 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size before optimization: 4\n",
      "2026-02-18 20:42:58,512 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MultiQueryOptimizer - MR plan size after optimization: 4\n",
      "2026-02-18 20:42:58,890 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.MRScriptState - Pig script settings are added to the job\n",
      "2026-02-18 20:42:58,899 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2026-02-18 20:42:58,900 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2026-02-18 20:42:58,902 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.output.compress is deprecated. Instead, use mapreduce.output.fileoutputformat.compress\n",
      "2026-02-18 20:42:58,905 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # of required reducers.\n",
      "2026-02-18 20:42:58,906 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator\n",
      "2026-02-18 20:42:58,947 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=534792\n",
      "2026-02-18 20:42:58,948 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2026-02-18 20:42:58,948 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2026-02-18 20:42:58,964 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2026-02-18 20:42:58,977 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Key [pig.schematuple] is false, will not generate code.\n",
      "2026-02-18 20:42:58,977 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Starting process to move generated code to distributed cacche\n",
      "2026-02-18 20:42:58,977 [main] INFO  org.apache.pig.data.SchemaTupleFrontend - Distributed cache not supported or needed in local mode. Setting key [pig.schematuple.local.dir] with code temp directory: /tmp/1771447378977-0\n",
      "2026-02-18 20:42:59,186 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2026-02-18 20:42:59,201 [JobControl] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:42:59,225 [JobControl] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id\n",
      "2026-02-18 20:42:59,378 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2026-02-18 20:42:59,408 [JobControl] INFO  org.apache.pig.builtin.PigStorage - Using PigTextInputFormat\n",
      "2026-02-18 20:42:59,434 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2026-02-18 20:42:59,436 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2026-02-18 20:42:59,457 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
      "2026-02-18 20:42:59,598 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2026-02-18 20:43:00,042 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local772871473_0001\n",
      "2026-02-18 20:43:00,046 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []\n",
      "2026-02-18 20:43:00,605 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n",
      "2026-02-18 20:43:00,606 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local772871473_0001\n",
      "2026-02-18 20:43:00,606 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases race_count,race_group,survey_clean,survey_valid\n",
      "2026-02-18 20:43:00,606 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: survey_clean[3,15],survey_clean[-1,-1],survey_valid[21,15],race_count[24,13],race_group[23,13] C: race_count[24,13],race_group[23,13] R: race_count[24,13]\n",
      "2026-02-18 20:43:00,611 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n",
      "2026-02-18 20:43:00,657 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 0% complete\n",
      "2026-02-18 20:43:00,658 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_local772871473_0001]\n",
      "2026-02-18 20:43:00,715 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2026-02-18 20:43:00,718 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2026-02-18 20:43:00,718 [Thread-5] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2026-02-18 20:43:00,727 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:43:00,730 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:43:00,731 [Thread-5] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:43:00,732 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n",
      "2026-02-18 20:43:00,920 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n",
      "2026-02-18 20:43:00,921 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local772871473_0001_m_000000_0\n",
      "2026-02-18 20:43:00,983 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:43:00,983 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:43:00,984 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:43:01,043 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2026-02-18 20:43:01,072 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 534792\n",
      "Input split[0]:\n",
      "   Length = 534792\n",
      "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
      "   Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2026-02-18 20:43:01,112 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.builtin.PigStorage - Using PigTextInputFormat\n",
      "2026-02-18 20:43:01,122 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/content/data/pig_out/survey_clean/part-m-00000:0+534792\n",
      "2026-02-18 20:43:01,390 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2026-02-18 20:43:01,391 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2026-02-18 20:43:01,391 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2026-02-18 20:43:01,391 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2026-02-18 20:43:01,391 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2026-02-18 20:43:01,407 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2026-02-18 20:43:01,445 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n",
      "2026-02-18 20:43:01,452 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.data.SchemaTupleBackend - Key [pig.schematuple] was not set... will not generate code.\n",
      "2026-02-18 20:43:01,534 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: survey_clean[3,15],survey_clean[-1,-1],survey_valid[21,15],race_count[24,13],race_group[23,13] C: race_count[24,13],race_group[23,13] R: race_count[24,13]\n",
      "2026-02-18 20:43:02,395 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - \n",
      "2026-02-18 20:43:02,395 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2026-02-18 20:43:02,395 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2026-02-18 20:43:02,395 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 61243; bufvoid = 104857600\n",
      "2026-02-18 20:43:02,395 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 26199128(104796512); length = 15269/6553600\n",
      "2026-02-18 20:43:02,506 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigCombiner$Combine - Aliases being processed per job phase (AliasName[line,offset]): M: survey_clean[3,15],survey_clean[-1,-1],survey_valid[21,15],race_count[24,13],race_group[23,13] C: race_count[24,13],race_group[23,13] R: race_count[24,13]\n",
      "2026-02-18 20:43:02,621 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2026-02-18 20:43:02,666 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local772871473_0001_m_000000_0 is done. And is in the process of committing\n",
      "2026-02-18 20:43:02,674 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map\n",
      "2026-02-18 20:43:02,675 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local772871473_0001_m_000000_0' done.\n",
      "2026-02-18 20:43:02,698 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local772871473_0001_m_000000_0: Counters: 18\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=539420\n",
      "\t\tFILE: Number of bytes written=697598\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=3818\n",
      "\t\tMap output records=3818\n",
      "\t\tMap output bytes=61243\n",
      "\t\tMap output materialized bytes=323\n",
      "\t\tInput split bytes=379\n",
      "\t\tCombine input records=3818\n",
      "\t\tCombine output records=11\n",
      "\t\tSpilled Records=11\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=85\n",
      "\t\tTotal committed heap usage (bytes)=225443840\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "2026-02-18 20:43:02,698 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local772871473_0001_m_000000_0\n",
      "2026-02-18 20:43:02,701 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n",
      "2026-02-18 20:43:02,709 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks\n",
      "2026-02-18 20:43:02,710 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local772871473_0001_r_000000_0\n",
      "2026-02-18 20:43:02,755 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:43:02,755 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:43:02,755 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:43:02,761 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2026-02-18 20:43:02,770 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@10f44551\n",
      "2026-02-18 20:43:02,780 [pool-4-thread-1] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:02,847 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=734003200, maxSingleShuffleLimit=183500800, mergeThreshold=484442144, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2026-02-18 20:43:02,855 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local772871473_0001_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2026-02-18 20:43:02,985 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#1 about to shuffle output of map attempt_local772871473_0001_m_000000_0 decomp: 319 len: 323 to MEMORY\n",
      "2026-02-18 20:43:02,996 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 319 bytes from map-output for attempt_local772871473_0001_m_000000_0\n",
      "2026-02-18 20:43:03,001 [localfetcher#1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 319, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->319\n",
      "2026-02-18 20:43:03,008 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning\n",
      "2026-02-18 20:43:03,011 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2026-02-18 20:43:03,012 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2026-02-18 20:43:03,033 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2026-02-18 20:43:03,033 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 309 bytes\n",
      "2026-02-18 20:43:03,036 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 1 segments, 319 bytes to disk to satisfy reduce memory limit\n",
      "2026-02-18 20:43:03,037 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 323 bytes from disk\n",
      "2026-02-18 20:43:03,037 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce\n",
      "2026-02-18 20:43:03,038 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2026-02-18 20:43:03,043 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 309 bytes\n",
      "2026-02-18 20:43:03,043 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2026-02-18 20:43:03,057 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:43:03,057 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:43:03,057 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:43:03,063 [pool-4-thread-1] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.skip.on is deprecated. Instead, use mapreduce.job.skiprecords\n",
      "2026-02-18 20:43:03,064 [pool-4-thread-1] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n",
      "2026-02-18 20:43:03,065 [pool-4-thread-1] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2026-02-18 20:43:03,080 [pool-4-thread-1] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce - Aliases being processed per job phase (AliasName[line,offset]): M: survey_clean[3,15],survey_clean[-1,-1],survey_valid[21,15],race_count[24,13],race_group[23,13] C: race_count[24,13],race_group[23,13] R: race_count[24,13]\n",
      "2026-02-18 20:43:03,103 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local772871473_0001_r_000000_0 is done. And is in the process of committing\n",
      "2026-02-18 20:43:03,119 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2026-02-18 20:43:03,119 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local772871473_0001_r_000000_0 is allowed to commit now\n",
      "2026-02-18 20:43:03,135 [pool-4-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local772871473_0001_r_000000_0' to file:/tmp/temp-2093896242/tmp-1238894952\n",
      "2026-02-18 20:43:03,136 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2026-02-18 20:43:03,137 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local772871473_0001_r_000000_0' done.\n",
      "2026-02-18 20:43:03,137 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local772871473_0001_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=540098\n",
      "\t\tFILE: Number of bytes written=698228\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=11\n",
      "\t\tReduce shuffle bytes=323\n",
      "\t\tReduce input records=11\n",
      "\t\tReduce output records=11\n",
      "\t\tSpilled Records=11\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=225443840\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "2026-02-18 20:43:03,138 [pool-4-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local772871473_0001_r_000000_0\n",
      "2026-02-18 20:43:03,138 [Thread-5] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.\n",
      "2026-02-18 20:43:03,165 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 25% complete\n",
      "2026-02-18 20:43:03,174 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:03,209 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:03,211 [main] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "2026-02-18 20:43:03,217 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:03,266 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.MRScriptState - Pig script settings are added to the job\n",
      "2026-02-18 20:43:03,267 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2026-02-18 20:43:03,268 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # of required reducers.\n",
      "2026-02-18 20:43:03,268 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Using reducer estimator: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator\n",
      "2026-02-18 20:43:03,272 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.InputSizeReducerEstimator - BytesPerReducer=1000000000 maxReducers=999 totalInputFileSize=295\n",
      "2026-02-18 20:43:03,273 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2026-02-18 20:43:03,278 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2026-02-18 20:43:03,324 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2026-02-18 20:43:03,329 [JobControl] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:03,345 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2026-02-18 20:43:03,357 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2026-02-18 20:43:03,357 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2026-02-18 20:43:03,357 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
      "2026-02-18 20:43:03,368 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2026-02-18 20:43:03,415 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1771019515_0002\n",
      "2026-02-18 20:43:03,415 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []\n",
      "2026-02-18 20:43:03,662 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n",
      "2026-02-18 20:43:03,662 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n",
      "2026-02-18 20:43:03,680 [Thread-14] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2026-02-18 20:43:03,680 [Thread-14] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2026-02-18 20:43:03,681 [Thread-14] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2026-02-18 20:43:03,681 [Thread-14] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:43:03,681 [Thread-14] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:43:03,681 [Thread-14] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:43:03,682 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n",
      "2026-02-18 20:43:03,693 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1771019515_0002_m_000000_0\n",
      "2026-02-18 20:43:03,693 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n",
      "2026-02-18 20:43:03,714 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:43:03,714 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:43:03,714 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:43:03,715 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2026-02-18 20:43:03,717 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 295\n",
      "Input split[0]:\n",
      "   Length = 295\n",
      "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
      "   Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2026-02-18 20:43:03,727 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/tmp/temp-2093896242/tmp-1238894952/part-r-00000:0+295\n",
      "2026-02-18 20:43:03,780 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2026-02-18 20:43:03,780 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2026-02-18 20:43:03,780 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2026-02-18 20:43:03,780 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2026-02-18 20:43:03,780 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2026-02-18 20:43:03,789 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2026-02-18 20:43:03,796 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n",
      "2026-02-18 20:43:03,796 [LocalJobRunner Map Task Executor #0] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2026-02-18 20:43:03,798 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Map - Aliases being processed per job phase (AliasName[line,offset]): M: race_order[25,13] C:  R: \n",
      "2026-02-18 20:43:03,801 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - \n",
      "2026-02-18 20:43:03,801 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2026-02-18 20:43:03,801 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Spilling map output\n",
      "2026-02-18 20:43:03,801 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufend = 180; bufvoid = 104857600\n",
      "2026-02-18 20:43:03,801 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396(104857584); kvend = 26214356(104857424); length = 41/6553600\n",
      "2026-02-18 20:43:03,804 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Finished spill 0\n",
      "2026-02-18 20:43:03,807 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1771019515_0002_m_000000_0 is done. And is in the process of committing\n",
      "2026-02-18 20:43:03,809 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - map\n",
      "2026-02-18 20:43:03,809 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1771019515_0002_m_000000_0' done.\n",
      "2026-02-18 20:43:03,809 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1771019515_0002_m_000000_0: Counters: 17\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=540845\n",
      "\t\tFILE: Number of bytes written=1379950\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=11\n",
      "\t\tMap output records=11\n",
      "\t\tMap output bytes=180\n",
      "\t\tMap output materialized bytes=208\n",
      "\t\tInput split bytes=380\n",
      "\t\tCombine input records=0\n",
      "\t\tSpilled Records=11\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tGC time elapsed (ms)=8\n",
      "\t\tTotal committed heap usage (bytes)=192937984\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "2026-02-18 20:43:03,810 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1771019515_0002_m_000000_0\n",
      "2026-02-18 20:43:03,810 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n",
      "2026-02-18 20:43:03,811 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for reduce tasks\n",
      "2026-02-18 20:43:03,812 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1771019515_0002_r_000000_0\n",
      "2026-02-18 20:43:03,828 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local1771019515_0002\n",
      "2026-02-18 20:43:03,828 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases race_order\n",
      "2026-02-18 20:43:03,829 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: race_order[25,13] C:  R: \n",
      "2026-02-18 20:43:03,833 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 37% complete\n",
      "2026-02-18 20:43:03,834 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Running jobs are [job_local1771019515_0002]\n",
      "2026-02-18 20:43:03,843 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:43:03,844 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:43:03,844 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:43:03,850 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2026-02-18 20:43:03,850 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.ReduceTask - Using ShuffleConsumerPlugin: org.apache.hadoop.mapreduce.task.reduce.Shuffle@51f9e2e8\n",
      "2026-02-18 20:43:03,851 [pool-9-thread-1] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:03,854 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - MergerManager: memoryLimit=734003200, maxSingleShuffleLimit=183500800, mergeThreshold=484442144, ioSortFactor=10, memToMemMergeOutputsThreshold=10\n",
      "2026-02-18 20:43:03,855 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - attempt_local1771019515_0002_r_000000_0 Thread started: EventFetcher for fetching Map Completion Events\n",
      "2026-02-18 20:43:03,858 [localfetcher#2] INFO  org.apache.hadoop.mapreduce.task.reduce.LocalFetcher - localfetcher#2 about to shuffle output of map attempt_local1771019515_0002_m_000000_0 decomp: 204 len: 208 to MEMORY\n",
      "2026-02-18 20:43:03,859 [localfetcher#2] INFO  org.apache.hadoop.mapreduce.task.reduce.InMemoryMapOutput - Read 204 bytes from map-output for attempt_local1771019515_0002_m_000000_0\n",
      "2026-02-18 20:43:03,860 [localfetcher#2] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - closeInMemoryFile -> map-output of size: 204, inMemoryMapOutputs.size() -> 1, commitMemory -> 0, usedMemory ->204\n",
      "2026-02-18 20:43:03,860 [EventFetcher for fetching Map Completion Events] INFO  org.apache.hadoop.mapreduce.task.reduce.EventFetcher - EventFetcher is interrupted.. Returning\n",
      "2026-02-18 20:43:03,861 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2026-02-18 20:43:03,861 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - finalMerge called with 1 in-memory map-outputs and 0 on-disk map-outputs\n",
      "2026-02-18 20:43:03,864 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2026-02-18 20:43:03,864 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 190 bytes\n",
      "2026-02-18 20:43:03,867 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merged 1 segments, 204 bytes to disk to satisfy reduce memory limit\n",
      "2026-02-18 20:43:03,868 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 1 files, 208 bytes from disk\n",
      "2026-02-18 20:43:03,868 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.task.reduce.MergeManagerImpl - Merging 0 segments, 0 bytes from memory into reduce\n",
      "2026-02-18 20:43:03,868 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Merger - Merging 1 sorted segments\n",
      "2026-02-18 20:43:03,869 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Merger - Down to the last merge-pass, with 1 segments left of total size: 190 bytes\n",
      "2026-02-18 20:43:03,870 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2026-02-18 20:43:03,876 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:43:03,876 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:43:03,876 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:43:03,878 [pool-9-thread-1] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n",
      "2026-02-18 20:43:03,879 [pool-9-thread-1] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2026-02-18 20:43:03,884 [pool-9-thread-1] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Reduce - Aliases being processed per job phase (AliasName[line,offset]): M: race_order[25,13] C:  R: \n",
      "2026-02-18 20:43:03,892 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Task - Task:attempt_local1771019515_0002_r_000000_0 is done. And is in the process of committing\n",
      "2026-02-18 20:43:03,900 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - 1 / 1 copied.\n",
      "2026-02-18 20:43:03,900 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Task - Task attempt_local1771019515_0002_r_000000_0 is allowed to commit now\n",
      "2026-02-18 20:43:03,909 [pool-9-thread-1] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - Saved output of task 'attempt_local1771019515_0002_r_000000_0' to file:/tmp/temp-2093896242/tmp-1689717013\n",
      "2026-02-18 20:43:03,910 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce > reduce\n",
      "2026-02-18 20:43:03,910 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Task - Task 'attempt_local1771019515_0002_r_000000_0' done.\n",
      "2026-02-18 20:43:03,911 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.Task - Final Counters for attempt_local1771019515_0002_r_000000_0: Counters: 24\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=541293\n",
      "\t\tFILE: Number of bytes written=1380219\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\tMap-Reduce Framework\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce shuffle bytes=208\n",
      "\t\tReduce input records=11\n",
      "\t\tReduce output records=1\n",
      "\t\tSpilled Records=11\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=0\n",
      "\t\tTotal committed heap usage (bytes)=192937984\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "2026-02-18 20:43:03,911 [pool-9-thread-1] INFO  org.apache.hadoop.mapred.LocalJobRunner - Finishing task: attempt_local1771019515_0002_r_000000_0\n",
      "2026-02-18 20:43:03,913 [Thread-14] INFO  org.apache.hadoop.mapred.LocalJobRunner - reduce task executor complete.\n",
      "2026-02-18 20:43:04,064 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 50% complete\n",
      "2026-02-18 20:43:04,069 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:04,072 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:04,074 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:04,080 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.MRScriptState - Pig script settings are added to the job\n",
      "2026-02-18 20:43:04,080 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - mapred.job.reduce.markreset.buffer.percent is not set, set to default 0.3\n",
      "2026-02-18 20:43:04,081 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Reduce phase detected, estimating # of required reducers.\n",
      "2026-02-18 20:43:04,081 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting Parallelism to 1\n",
      "2026-02-18 20:43:04,084 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler - Setting up single store job\n",
      "2026-02-18 20:43:04,115 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 1 map-reduce job(s) waiting for submission.\n",
      "2026-02-18 20:43:04,119 [JobControl] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:04,134 [JobControl] WARN  org.apache.hadoop.mapreduce.JobResourceUploader - No job jar file set.  User classes may not be found. See Job or Job#setJar(String).\n",
      "2026-02-18 20:43:04,143 [JobControl] INFO  org.apache.hadoop.mapreduce.lib.input.FileInputFormat - Total input files to process : 1\n",
      "2026-02-18 20:43:04,143 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1\n",
      "2026-02-18 20:43:04,143 [JobControl] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths (combined) to process : 1\n",
      "2026-02-18 20:43:04,153 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - number of splits:1\n",
      "2026-02-18 20:43:04,176 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Submitting tokens for job: job_local1921962886_0003\n",
      "2026-02-18 20:43:04,176 [JobControl] INFO  org.apache.hadoop.mapreduce.JobSubmitter - Executing with tokens: []\n",
      "2026-02-18 20:43:04,343 [JobControl] INFO  org.apache.hadoop.mapreduce.Job - The url to track the job: http://localhost:8080/\n",
      "2026-02-18 20:43:04,343 [Thread-21] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter set in config null\n",
      "2026-02-18 20:43:04,376 [Thread-21] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "2026-02-18 20:43:04,376 [Thread-21] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.tracker is deprecated. Instead, use mapreduce.jobtracker.address\n",
      "2026-02-18 20:43:04,376 [Thread-21] INFO  org.apache.hadoop.conf.Configuration.deprecation - mapred.job.reduce.markreset.buffer.percent is deprecated. Instead, use mapreduce.reduce.markreset.buffer.percent\n",
      "2026-02-18 20:43:04,377 [Thread-21] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:43:04,377 [Thread-21] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:43:04,377 [Thread-21] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:43:04,379 [Thread-21] INFO  org.apache.hadoop.mapred.LocalJobRunner - OutputCommitter is org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputCommitter\n",
      "2026-02-18 20:43:04,395 [Thread-21] INFO  org.apache.hadoop.mapred.LocalJobRunner - Waiting for map tasks\n",
      "2026-02-18 20:43:04,396 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.LocalJobRunner - Starting task: attempt_local1921962886_0003_m_000000_0\n",
      "2026-02-18 20:43:04,416 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.PathOutputCommitterFactory - No output committer factory defined, defaulting to FileOutputCommitterFactory\n",
      "2026-02-18 20:43:04,416 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - File Output Committer Algorithm version is 2\n",
      "2026-02-18 20:43:04,416 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapreduce.lib.output.FileOutputCommitter - FileOutputCommitter skip cleanup _temporary folders under output directory:false, ignore cleanup failures: false\n",
      "2026-02-18 20:43:04,417 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.Task -  Using ResourceCalculatorProcessTree : [ ]\n",
      "2026-02-18 20:43:04,420 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Processing split: Number of splits :1\n",
      "Total Length = 295\n",
      "Input split[0]:\n",
      "   Length = 295\n",
      "   ClassName: org.apache.hadoop.mapreduce.lib.input.FileSplit\n",
      "   Locations:\n",
      "\n",
      "-----------------------\n",
      "\n",
      "2026-02-18 20:43:04,426 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current split being processed file:/tmp/temp-2093896242/tmp-1238894952/part-r-00000:0+295\n",
      "2026-02-18 20:43:04,454 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - (EQUATOR) 0 kvi 26214396(104857584)\n",
      "2026-02-18 20:43:04,454 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - mapreduce.task.io.sort.mb: 100\n",
      "2026-02-18 20:43:04,454 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - soft limit at 83886080\n",
      "2026-02-18 20:43:04,454 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - bufstart = 0; bufvoid = 104857600\n",
      "2026-02-18 20:43:04,454 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - kvstart = 26214396; length = 6553600\n",
      "2026-02-18 20:43:04,457 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Map output collector class = org.apache.hadoop.mapred.MapTask$MapOutputBuffer\n",
      "2026-02-18 20:43:04,461 [LocalJobRunner Map Task Executor #0] INFO  org.apache.pig.impl.util.SpillableMemoryManager - Selected heap (G1 Old Gen) of size 1048576000 to monitor. collectionUsageThreshold = 734003200, usageThreshold = 734003200\n",
      "2026-02-18 20:43:04,462 [LocalJobRunner Map Task Executor #0] WARN  org.apache.pig.data.SchemaTupleBackend - SchemaTupleBackend has already been initialized\n",
      "2026-02-18 20:43:04,464 [LocalJobRunner Map Task Executor #0] INFO  org.apache.hadoop.mapred.MapTask - Starting flush of map output\n",
      "2026-02-18 20:43:04,469 [Thread-21] INFO  org.apache.hadoop.mapred.LocalJobRunner - map task executor complete.\n",
      "2026-02-18 20:43:04,484 [Thread-21] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local1921962886_0003\n",
      "java.lang.Exception: java.io.IOException: Deserialization error: Cannot invoke \"org.apache.pig.impl.plan.OperatorKey.hashCode()\" because \"this.mKey\" is null\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.runTasks(LocalJobRunner.java:492)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:552)\n",
      "Caused by: java.io.IOException: Deserialization error: Cannot invoke \"org.apache.pig.impl.plan.OperatorKey.hashCode()\" because \"this.mKey\" is null\n",
      "\tat org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:62)\n",
      "\tat org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapBase.setup(PigGenericMapBase.java:183)\n",
      "\tat org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:142)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:800)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:348)\n",
      "\tat org.apache.hadoop.mapred.LocalJobRunner$Job$MapTaskRunnable.run(LocalJobRunner.java:271)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:539)\n",
      "\tat java.base/java.util.concurrent.FutureTask.run(FutureTask.java:264)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1136)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:635)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:840)\n",
      "Caused by: java.lang.NullPointerException: Cannot invoke \"org.apache.pig.impl.plan.OperatorKey.hashCode()\" because \"this.mKey\" is null\n",
      "\tat org.apache.pig.impl.plan.Operator.hashCode(Operator.java:106)\n",
      "\tat java.base/java.util.HashMap.hash(HashMap.java:338)\n",
      "\tat java.base/java.util.HashMap.readObject(HashMap.java:1553)\n",
      "\tat java.base/jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1100)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2423)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat java.base/java.util.ArrayList.readObject(ArrayList.java:899)\n",
      "\tat java.base/jdk.internal.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1100)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2423)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat java.base/java.util.ArrayList.readObject(ArrayList.java:899)\n",
      "\tat java.base/jdk.internal.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1100)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2423)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat java.base/java.util.HashMap.readObject(HashMap.java:1552)\n",
      "\tat java.base/jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1100)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2423)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat java.base/java.util.ArrayList.readObject(ArrayList.java:899)\n",
      "\tat java.base/jdk.internal.reflect.GeneratedMethodAccessor3.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1100)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2423)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat java.base/java.util.HashMap.readObject(HashMap.java:1552)\n",
      "\tat java.base/jdk.internal.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)\n",
      "\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.base/java.lang.reflect.Method.invoke(Method.java:569)\n",
      "\tat java.base/java.io.ObjectStreamClass.invokeReadObject(ObjectStreamClass.java:1100)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2423)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream$FieldValues.<init>(ObjectInputStream.java:2606)\n",
      "\tat java.base/java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:2457)\n",
      "\tat java.base/java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:2257)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1733)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:509)\n",
      "\tat java.base/java.io.ObjectInputStream.readObject(ObjectInputStream.java:467)\n",
      "\tat org.apache.pig.impl.util.ObjectSerializer.deserialize(ObjectSerializer.java:60)\n",
      "\t... 10 more\n",
      "2026-02-18 20:43:04,648 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - HadoopJobId: job_local1921962886_0003\n",
      "2026-02-18 20:43:04,648 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Processing aliases race_order\n",
      "2026-02-18 20:43:04,648 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - detailed locations: M: race_order[25,13] C:  R: \n",
      "2026-02-18 20:43:04,652 [main] WARN  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Ooops! Some job has failed! Specify -stop_on_failure if you want Pig to stop immediately on failure.\n",
      "2026-02-18 20:43:04,652 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - job job_local1921962886_0003 has failed! Stop running all dependent jobs\n",
      "2026-02-18 20:43:04,652 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - 100% complete\n",
      "2026-02-18 20:43:04,658 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:04,663 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:04,666 [main] ERROR org.apache.pig.tools.pigstats.mapreduce.MRPigStatsUtil - 1 map reduce job(s) failed!\n",
      "2026-02-18 20:43:04,674 [main] INFO  org.apache.pig.tools.pigstats.mapreduce.SimplePigStats - Script Statistics: \n",
      "\n",
      "HadoopVersion\tPigVersion\tUserId\tStartedAt\tFinishedAt\tFeatures\n",
      "3.4.2\t0.17.0\troot\t2026-02-18 20:42:58\t2026-02-18 20:43:04\tGROUP_BY,ORDER_BY,FILTER,LIMIT\n",
      "\n",
      "Some jobs have failed! Stop running all dependent jobs\n",
      "\n",
      "Job Stats (time in seconds):\n",
      "JobId\tMaps\tReduces\tMaxMapTime\tMinMapTime\tAvgMapTime\tMedianMapTime\tMaxReduceTime\tMinReduceTime\tAvgReduceTime\tMedianReducetime\tAlias\tFeature\tOutputs\n",
      "job_local1771019515_0002\t1\t1\tn/a\tn/a\tn/a\tn/a\tn/a\tn/a\tn/a\tn/a\trace_order\tSAMPLER\t\n",
      "job_local772871473_0001\t1\t1\tn/a\tn/a\tn/a\tn/a\tn/a\tn/a\tn/a\tn/a\trace_count,race_group,survey_clean,survey_valid\tGROUP_BY,COMBINER\t\n",
      "\n",
      "Failed Jobs:\n",
      "JobId\tAlias\tFeature\tMessage\tOutputs\n",
      "job_local1921962886_0003\trace_order\tORDER_BY,COMBINER\tMessage: Job failed!\t\n",
      "\n",
      "Input(s):\n",
      "Successfully read 3818 records from: \"/content/data/pig_out/survey_clean\"\n",
      "\n",
      "Output(s):\n",
      "\n",
      "Counters:\n",
      "Total records written : 0\n",
      "Total bytes written : 0\n",
      "Spillable Memory Manager spill count : 0\n",
      "Total bags proactively spilled: 0\n",
      "Total records proactively spilled: 0\n",
      "\n",
      "Job DAG:\n",
      "job_local772871473_0001\t->\tjob_local1771019515_0002,\n",
      "job_local1771019515_0002\t->\tjob_local1921962886_0003,\n",
      "job_local1921962886_0003\t->\tnull,\n",
      "null\n",
      "\n",
      "\n",
      "2026-02-18 20:43:04,677 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:04,683 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:04,688 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:04,706 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:04,710 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:04,713 [main] WARN  org.apache.hadoop.metrics2.impl.MetricsSystemImpl - JobTracker metrics system already initialized!\n",
      "2026-02-18 20:43:04,721 [main] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher - Some jobs have failed! Stop running all dependent jobs\n",
      "2026-02-18 20:43:04,728 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 1066: Unable to open iterator for alias top3_race\n",
      "Details at logfile: /content/pig_1771447376833.log\n",
      "2026-02-18 20:43:04,793 [main] INFO  org.apache.pig.Main - Pig script completed in 8 seconds and 362 milliseconds (8362 ms)\n",
      "\n",
      "Verificaci√≥n r√°pida 3.2:\n",
      "- Deben aparecer 3 filas con formato (RACE,conteo) en el DUMP\n",
      "- Debe aparecer 'Success!' al final del log\n"
     ]
    }
   ],
   "source": [
    "# Ejecutar tratamiento 3.2 (Top 3 RACE) y dejar pista clara de √©xito\n",
    "\n",
    "!pig -x local -f tratamiento_top3_race.pig | tee top3_race.log\n",
    "\n",
    "print(\"\\nVerificaci√≥n r√°pida 3.2:\")\n",
    "print(\"- Deben aparecer 3 filas con formato (RACE,conteo) en el DUMP\")\n",
    "print(\"- Debe aparecer 'Success!' al final del log\")\n",
    "\n",
    "!grep -n \"Success!\" top3_race.log | tail -n 1\n",
    "!grep -E \"^\\(.*,[0-9]+\\)$\" top3_race.log | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64049fc5",
   "metadata": {
    "id": "64049fc5"
   },
   "source": [
    "## Conclusiones (Secci√≥n 3.2)\n",
    "He ejecutado el script tratamiento_top3_race.pig en modo local y he mostrado el resultado directamente en pantalla usando DUMP, lo que deja evidencia visible en el cuaderno.\n",
    "Como tratamiento interesante, he calculado el Top 3 de valores m√°s frecuentes de la variable RACE a partir de los datos ya limpiados (survey_clean). El resultado obtenido es:\n",
    "\n",
    "White: 3313\n",
    "\n",
    "Black or African American: 253\n",
    "\n",
    "Asian: 120\n",
    "\n",
    "Este paso completa el apartado 3.2 con una agregaci√≥n sencilla (GROUP + COUNT + ORDER + LIMIT) y deja el dataset preparado para continuar con Spark/Hive."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "325dffca",
   "metadata": {},
   "source": [
    "## 4) Spark (PySpark) ‚Üí Hive\n",
    "En esta secci√≥n voy a usar PySpark para leer los datos limpios que gener√© con Pig.\n",
    "Como el output de Pig no trae cabecera garantizada, aplicar√© el esquema usando las cabeceras de los CSV originales.\n",
    "Despu√©s guardar√© los dos DataFrames en Hive como `tabla_survey` y `tabla_genotyping`.\n",
    "As√≠ dejo preparadas dos tablas relacionales para poder hacer JOIN real en la Secci√≥n 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac6f32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secci√≥n 4 ‚Äî Configurar Spark con soporte Hive\n",
    "\n",
    "import os\n",
    "\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-17-openjdk-amd64\"\n",
    "\n",
    "!pip -q install pyspark findspark\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"BDA03_Spark_Hive\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/content/spark-warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"spark.version:\", spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03e4894",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Secci√≥n 4 ‚Äî Cargar datos limpios y crear tablas Hive\n",
    "\n",
    "import csv\n",
    "from pyspark.sql.functions import col, trim\n",
    "\n",
    "# 1) Cabeceras desde CSV originales\n",
    "with open('/content/data/csv/survey.csv', 'r', encoding='utf-8') as f:\n",
    "    survey_headers = next(csv.reader(f))\n",
    "\n",
    "with open('/content/data/csv/genotyping.csv', 'r', encoding='utf-8') as f:\n",
    "    genotyping_headers = next(csv.reader(f))\n",
    "\n",
    "# Nombre v√°lido para Spark/Hive (en original viene \"contact key\")\n",
    "genotyping_headers = [h.replace('contact key', 'contact_key') for h in genotyping_headers]\n",
    "\n",
    "# 2) Leer output Pig (sin cabecera) y aplicar esquema\n",
    "survey_df = spark.read.option('header', 'false').option('inferSchema', 'false').csv('/content/data/pig_out/survey_clean') \\\n",
    "    .toDF(*survey_headers)\n",
    "\n",
    "genotyping_df = spark.read.option('header', 'false').option('inferSchema', 'false').csv('/content/data/pig_out/genotyping_clean') \\\n",
    "    .toDF(*genotyping_headers)\n",
    "\n",
    "# 3) Asegurar claves de JOIN como string y normalizadas\n",
    "survey_df = survey_df.withColumn('SUBJECT_ID', trim(col('SUBJECT_ID').cast('string')))\n",
    "genotyping_df = genotyping_df.withColumn('FID', trim(col('FID').cast('string')))\n",
    "\n",
    "# 4) Evidencia m√≠nima: esquema y muestra\n",
    "print('Schema tabla_survey:')\n",
    "survey_df.printSchema()\n",
    "print('Muestra tabla_survey:')\n",
    "survey_df.show(3, truncate=False)\n",
    "\n",
    "print('Schema tabla_genotyping:')\n",
    "genotyping_df.printSchema()\n",
    "print('Muestra tabla_genotyping:')\n",
    "genotyping_df.show(3, truncate=False)\n",
    "\n",
    "# 5) Guardar en Hive (2 tablas)\n",
    "survey_df.write.mode('overwrite').saveAsTable('tabla_survey')\n",
    "genotyping_df.write.mode('overwrite').saveAsTable('tabla_genotyping')\n",
    "\n",
    "# 6) Evidencia m√≠nima en Hive\n",
    "print('Tablas Hive creadas:')\n",
    "spark.sql('SHOW TABLES').show(truncate=False)\n",
    "\n",
    "print('COUNT tabla_survey:')\n",
    "spark.sql('SELECT COUNT(*) AS total FROM tabla_survey').show()\n",
    "print('COUNT tabla_genotyping:')\n",
    "spark.sql('SELECT COUNT(*) AS total FROM tabla_genotyping').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd48fd5",
   "metadata": {},
   "source": [
    "## Conclusiones (Secci√≥n 4 ‚Äî Spark ‚Üí Hive)\n",
    "\n",
    "He cargado en Spark los dos ficheros limpios que ven√≠an de Pig y los he guardado en Hive como **dos tablas** para poder hacer JOIN en el siguiente apartado.\n",
    "\n",
    "- La tabla **`tabla_survey`** se ha creado con la clave **`SUBJECT_ID`** y el resto de campos cl√≠nicos (15 columnas). La muestra de filas confirma que los datos se leen bien y que Spark reconoce valores faltantes (por ejemplo, `Risk` aparece como `NULL` en una de las filas mostradas).\n",
    "- La tabla **`tabla_genotyping`** se ha creado con la clave **`FID`**, `contact_key` y todos los marcadores gen√©ticos `rs...`. La muestra confirma el formato esperado de los genotipos (valores como `A:G`, `C:C`, etc.).\n",
    "\n",
    "He verificado que ambas tablas existen en Hive con `SHOW TABLES` y aparecen como tablas no temporales: **`tabla_survey`** y **`tabla_genotyping`**.\n",
    "\n",
    "Por √∫ltimo, he comprobado el volumen cargado en Hive con `COUNT(*)` y en ambos casos el resultado es **3818 filas**, as√≠ que la carga es consistente. Con esto, dejo listo el entorno para las consultas HQL con JOIN usando **`SUBJECT_ID` ‚Üî `FID`**.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
